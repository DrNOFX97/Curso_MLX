{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M√≥dulo 3: Treino de Modelos Pequenos e M√©dios\n",
    "\n",
    "### 3.1 Classifica√ß√£o de Imagens\n",
    "\n",
    "#### O que s√£o CNNs (Convolutional Neural Networks)?\n",
    "\n",
    "**CNN = Rede Neural Convolucional**\n",
    "\n",
    "S√£o um tipo especial de rede neural **desenhada especificamente para processar imagens**. S√£o a base de praticamente toda a vis√£o computacional moderna.\n",
    "\n",
    "**Por que CNNs existem?**\n",
    "\n",
    "Problema com redes neurais normais em imagens:\n",
    "- Imagina uma imagem pequena de 224√ó224 pixels RGB\n",
    "- 224 √ó 224 √ó 3 = **150,528 pixels**\n",
    "- Rede neural normal: cada neur√¥nio conecta-se a TODOS os pixels\n",
    "- Primeira camada com 1000 neur√¥nios = **150 milh√µes de par√¢metros!**\n",
    "- ‚ùå Imposs√≠vel de treinar, overfitting garantido\n",
    "\n",
    "Solu√ß√£o das CNNs:\n",
    "- ‚úÖ Usa **filtros locais** (olha pequenas regi√µes de cada vez)\n",
    "- ‚úÖ Partilha pesos (mesmo filtro para toda a imagem)\n",
    "- ‚úÖ Aprende hierarquia de features (bordas ‚Üí formas ‚Üí objetos)\n",
    "\n",
    "**Como funcionam as camadas convolucionais:**\n",
    "\n",
    "Um filtro 3√ó3 \"desliza\" pela imagem detectando padr√µes espec√≠ficos. Cada filtro aprende a detectar uma caracter√≠stica diferente (bordas verticais, horizontais, curvas, texturas, etc.).\n",
    "\n",
    "**Hierarquia de aprendizagem:**\n",
    "```\n",
    "Camada 1 ‚Üí Deteta bordas simples (/, \\, |, ‚Äï)\n",
    "Camada 2 ‚Üí Combina bordas em formas (c√≠rculos, quadrados)\n",
    "Camada 3 ‚Üí Combina formas em partes (olhos, rodas, janelas)\n",
    "Camada 4 ‚Üí Combina partes em objetos (gato, carro, casa)\n",
    "```\n",
    "\n",
    "**Arquitetura t√≠pica de uma CNN:**\n",
    "\n",
    "```python\n",
    "# cnn_basica.py\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# CNN simples para entender a estrutura\n",
    "model = keras.Sequential([\n",
    "    # BLOCO 1: Deteta features simples\n",
    "    keras.layers.Conv2D(32, (3,3), activation='relu', \n",
    "                        input_shape=(224,224,3), name='conv1'),\n",
    "    keras.layers.MaxPooling2D((2,2), name='pool1'),\n",
    "    \n",
    "    # BLOCO 2: Features mais complexas\n",
    "    keras.layers.Conv2D(64, (3,3), activation='relu', name='conv2'),\n",
    "    keras.layers.MaxPooling2D((2,2), name='pool2'),\n",
    "    \n",
    "    # BLOCO 3: Features de alto n√≠vel\n",
    "    keras.layers.Conv2D(128, (3,3), activation='relu', name='conv3'),\n",
    "    keras.layers.MaxPooling2D((2,2), name='pool3'),\n",
    "    \n",
    "    # Achatar para classifica√ß√£o\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation='softmax')  # 10 classes\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Mostra a progress√£o do tamanho:\n",
    "# Input:  224√ó224√ó3    (imagem original)\n",
    "# Conv1:  222√ó222√ó32   (32 filtros aprendidos)\n",
    "# Pool1:  111√ó111√ó32   (redu√ß√£o de tamanho)\n",
    "# Conv2:  109√ó109√ó64   (64 padr√µes mais complexos)\n",
    "# Pool2:  54√ó54√ó64     (redu√ß√£o)\n",
    "# Conv3:  52√ó52√ó128    (128 features abstratas)\n",
    "# Pool3:  26√ó26√ó128    (redu√ß√£o final)\n",
    "# Flatten: 86528       (vetor √∫nico)\n",
    "# Dense:  128          (combina√ß√£o de features)\n",
    "# Output: 10           (probabilidades das classes)\n",
    "```\n",
    "\n",
    "**Visualizar o que uma CNN aprende:**\n",
    "\n",
    "```python\n",
    "# visualizar_cnn.py\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualizar_filtros_cnn(caminho_imagem):\n",
    "    \"\"\"\n",
    "    Mostra o que cada camada de uma CNN deteta\n",
    "    \"\"\"\n",
    "    # Carregar modelo pr√©-treinado\n",
    "    model = keras.applications.MobileNetV2(weights='imagenet', include_top=False)\n",
    "    \n",
    "    # Carregar e preparar imagem\n",
    "    img = keras.preprocessing.image.load_img(caminho_imagem, target_size=(224, 224))\n",
    "    img_array = keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
    "    \n",
    "    # Criar modelo para ver ativa√ß√µes intermedi√°rias\n",
    "    layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "    activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)\n",
    "    \n",
    "    # Obter ativa√ß√µes\n",
    "    activations = activation_model.predict(img_array)\n",
    "    \n",
    "    # Visualizar primeira camada (features b√°sicas)\n",
    "    first_layer = activations[0]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(4, 9, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.title('Original')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Mostrar 32 filtros diferentes\n",
    "    for i in range(32):\n",
    "        plt.subplot(4, 9, i+2)\n",
    "        plt.imshow(first_layer[0, :, :, i], cmap='viridis')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'F{i+1}', fontsize=8)\n",
    "    \n",
    "    plt.suptitle('O que a CNN \"v√™\": 32 filtros da primeira camada', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('cnn_visualizacao.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úì Cada imagem colorida mostra um padr√£o diferente que a CNN detetou!\")\n",
    "    print(\"  - Alguns filtros detetam bordas verticais\")\n",
    "    print(\"  - Outros detetam bordas horizontais\")\n",
    "    print(\"  - Outros detetam cores ou texturas espec√≠ficas\")\n",
    "\n",
    "# Uso:\n",
    "# visualizar_filtros_cnn('gato.jpg')\n",
    "```\n",
    "\n",
    "**Tipos de opera√ß√µes em CNNs:**\n",
    "\n",
    "1. **Convolu√ß√£o**: Deteta padr√µes locais\n",
    "2. **Pooling**: Reduz dimensionalidade mantendo informa√ß√£o importante\n",
    "3. **Batch Normalization**: Estabiliza o treino\n",
    "4. **Dropout**: Previne overfitting\n",
    "\n",
    "Agora que entendes CNNs, vamos aos modelos pr√°ticos otimizados para o M1!\n",
    "\n",
    "#### CNNs Compactas (MobileNet, EfficientNet)\n",
    "\n",
    "Modelos leves e eficientes, perfeitos para o M1 16GB:\n",
    "\n",
    "**Por que usar modelos compactos?**\n",
    "- ‚úÖ Treinam mais r√°pido\n",
    "- ‚úÖ Usam menos mem√≥ria\n",
    "- ‚úÖ Performance surpreendentemente boa\n",
    "- ‚úÖ Ideais para dispositivos m√≥veis/edge\n",
    "\n",
    "**Compara√ß√£o de Modelos:**\n",
    "\n",
    "| Modelo | Par√¢metros | Tamanho | Top-1 Accuracy | Ideal para M1? |\n",
    "|--------|-----------|---------|----------------|----------------|\n",
    "| MobileNetV2 | 3.5M | 14MB | 71.3% | ‚úÖ Excelente |\n",
    "| EfficientNetB0 | 5.3M | 29MB | 77.1% | ‚úÖ Excelente |\n",
    "| ResNet50 | 25.6M | 98MB | 76.1% | ‚úÖ Bom |\n",
    "| EfficientNetB4 | 19M | 75MB | 83.0% | ‚ö†Ô∏è Usar batch pequeno |\n",
    "| ResNet152 | 60.2M | 232MB | 78.3% | ‚ö†Ô∏è Limite do M1 |\n",
    "\n",
    "#### Exemplo Pr√°tico: Classificador de Imagens Custom\n",
    "\n",
    "**Cen√°rio:** Criar classificador de 10 categorias de animais.\n",
    "\n",
    "```python\n",
    "# classificador_animais.py\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "def criar_modelo_mobilenet(num_classes=10, input_shape=(224, 224, 3)):\n",
    "    \"\"\"\n",
    "    Modelo baseado em MobileNetV2 para classifica√ß√£o custom\n",
    "    \"\"\"\n",
    "    # Base pr√©-treinada (sem top layer)\n",
    "    base_model = keras.applications.MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Congelar base inicialmente (transfer learning)\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Adicionar camadas custom\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    x = keras.layers.Dense(128, activation='relu')(x)\n",
    "    x = keras.layers.Dropout(0.2)(x)\n",
    "    outputs = keras.layers.Dense(num_classes, activation='softmax', dtype='float32')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    # Compilar\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Criar modelo\n",
    "model, base_model = criar_modelo_mobilenet(num_classes=10)\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "**Preparar dados:**\n",
    "\n",
    "```python\n",
    "# preparar_dados.py\n",
    "def preparar_dataset(data_dir, img_size=(224, 224), batch_size=32):\n",
    "    \"\"\"\n",
    "    Prepara dataset com data augmentation\n",
    "    \"\"\"\n",
    "    # Data augmentation para treino\n",
    "    train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    # Apenas rescale para valida√ß√£o\n",
    "    val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        data_dir,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_generator, val_generator\n",
    "\n",
    "# Estrutura de pastas esperada:\n",
    "# data/\n",
    "#   ‚îú‚îÄ‚îÄ animals/\n",
    "#   ‚îÇ   ‚îú‚îÄ‚îÄ cat/\n",
    "#   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img1.jpg\n",
    "#   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ img2.jpg\n",
    "#   ‚îÇ   ‚îú‚îÄ‚îÄ dog/\n",
    "#   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "#### Transfer Learning com Modelos Pr√©-treinados\n",
    "\n",
    "**Fase 1: Treinar apenas o top (r√°pido)**\n",
    "\n",
    "```python\n",
    "# treino_fase1.py\n",
    "from tensorflow import keras\n",
    "\n",
    "def treino_fase1(model, train_gen, val_gen, epochs=10):\n",
    "    \"\"\"\n",
    "    Fase 1: Apenas camadas custom (base congelada)\n",
    "    \"\"\"\n",
    "    print(\"üéØ FASE 1: Treino do classificador (base congelada)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-7\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            'modelo_fase1.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Executar\n",
    "history1 = treino_fase1(model, train_generator, val_generator, epochs=10)\n",
    "```\n",
    "\n",
    "**Fase 2: Fine-tuning (descongelar algumas camadas)**\n",
    "\n",
    "```python\n",
    "# treino_fase2.py\n",
    "def treino_fase2(model, base_model, train_gen, val_gen, epochs=20):\n",
    "    \"\"\"\n",
    "    Fase 2: Fine-tuning de camadas superiores\n",
    "    \"\"\"\n",
    "    print(\"\\nüéØ FASE 2: Fine-tuning (descongelando camadas superiores)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Descongelar √∫ltimas 30 camadas da base\n",
    "    base_model.trainable = True\n",
    "    for layer in base_model.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Recompilar com learning rate menor\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(f\"Camadas trein√°veis: {sum([1 for l in model.layers if l.trainable])}\")\n",
    "    \n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            'modelo_final.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Executar\n",
    "history2 = treino_fase2(model, base_model, train_generator, val_generator, epochs=20)\n",
    "```\n",
    "\n",
    "#### Fine-tuning Eficiente\n",
    "\n",
    "**Script completo de treino otimizado para M1:**\n",
    "\n",
    "```python\n",
    "# treino_completo_m1.py\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import time\n",
    "\n",
    "# Ativar mixed precision\n",
    "policy = keras.mixed_precision.Policy('mixed_float16')\n",
    "keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "class TreinadorM1:\n",
    "    def __init__(self, num_classes, modelo_base='mobilenet'):\n",
    "        self.num_classes = num_classes\n",
    "        self.modelo_base = modelo_base\n",
    "        self.model = None\n",
    "        self.base_model = None\n",
    "        \n",
    "    def criar_modelo(self):\n",
    "        \"\"\"Cria modelo otimizado para M1\"\"\"\n",
    "        if self.modelo_base == 'mobilenet':\n",
    "            base = keras.applications.MobileNetV2(\n",
    "                include_top=False,\n",
    "                weights='imagenet',\n",
    "                input_shape=(224, 224, 3)\n",
    "            )\n",
    "        elif self.modelo_base == 'efficientnet':\n",
    "            base = keras.applications.EfficientNetB0(\n",
    "                include_top=False,\n",
    "                weights='imagenet',\n",
    "                input_shape=(224, 224, 3)\n",
    "            )\n",
    "        \n",
    "        base.trainable = False\n",
    "        \n",
    "        inputs = keras.Input(shape=(224, 224, 3))\n",
    "        x = base(inputs, training=False)\n",
    "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = keras.layers.Dropout(0.3)(x)\n",
    "        x = keras.layers.Dense(128, activation='relu')(x)\n",
    "        x = keras.layers.Dropout(0.2)(x)\n",
    "        outputs = keras.layers.Dense(\n",
    "            self.num_classes, \n",
    "            activation='softmax',\n",
    "            dtype='float32'  # Importante para mixed precision\n",
    "        )(x)\n",
    "        \n",
    "        model = keras.Model(inputs, outputs)\n",
    "        self.model = model\n",
    "        self.base_model = base\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def treinar_completo(self, train_gen, val_gen):\n",
    "        \"\"\"Pipeline completo de treino\"\"\"\n",
    "        \n",
    "        # FASE 1: Transfer Learning\n",
    "        print(\"üöÄ Iniciando Fase 1: Transfer Learning\")\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(1e-3),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        start = time.time()\n",
    "        hist1 = self.model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=10,\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "                keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=2)\n",
    "            ]\n",
    "        )\n",
    "        tempo_fase1 = time.time() - start\n",
    "        \n",
    "        # FASE 2: Fine-tuning\n",
    "        print(\"\\nüéØ Iniciando Fase 2: Fine-tuning\")\n",
    "        self.base_model.trainable = True\n",
    "        for layer in self.base_model.layers[:-30]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(1e-5),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        start = time.time()\n",
    "        hist2 = self.model.fit(\n",
    "            train_gen,\n",
    "            validation_data=val_gen,\n",
    "            epochs=15,\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "                keras.callbacks.ModelCheckpoint('melhor_modelo.keras', save_best_only=True)\n",
    "            ]\n",
    "        )\n",
    "        tempo_fase2 = time.time() - start\n",
    "        \n",
    "        print(f\"\\n‚úÖ Treino completo!\")\n",
    "        print(f\"Fase 1: {tempo_fase1/60:.1f} minutos\")\n",
    "        print(f\"Fase 2: {tempo_fase2/60:.1f} minutos\")\n",
    "        print(f\"Total: {(tempo_fase1+tempo_fase2)/60:.1f} minutos\")\n",
    "        \n",
    "        return hist1, hist2\n",
    "\n",
    "# Uso\n",
    "treinador = TreinadorM1(num_classes=10, modelo_base='mobilenet')\n",
    "treinador.criar_modelo()\n",
    "hist1, hist2 = treinador.treinar_completo(train_generator, val_generator)\n",
    "```\n",
    "\n",
    "--- \n",
    "\n",
    "### 3.2 Processamento de Linguagem Natural (NLP)\n",
    "\n",
    "#### Modelos Pequenos (DistilBERT, TinyBERT)\n",
    "\n",
    "Para NLP no M1 16GB, usa vers√µes destiladas:\n",
    "\n",
    "| Modelo | Par√¢metros | Tamanho | Performance | M1 16GB |\n",
    "|--------|-----------|---------|-------------|---------|\n",
    "| BERT-base | 110M | 440MB | 100% | ‚ö†Ô∏è Limite |\n",
    "| DistilBERT | 66M | 260MB | 97% | ‚úÖ Ideal |\n",
    "| TinyBERT | 14M | 56MB | 96% | ‚úÖ Excelente |\n",
    "| MobileBERT | 25M | 100MB | 99% | ‚úÖ Muito bom |\n",
    "\n",
    "**Instala√ß√£o:**\n",
    "\n",
    "```bash\n",
    "pip install transformers datasets accelerate\n",
    "```\n",
    "\n",
    "#### Treino de Embeddings\n",
    "\n",
    "**Exemplo: Classifica√ß√£o de Sentimentos**\n",
    "\n",
    "```python\n",
    "# classificador_sentimentos.py\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    TFDistilBertForSequenceClassification,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "class ClassificadorSentimentos:\n",
    "    def __init__(self, num_labels=2):\n",
    "        self.num_labels = num_labels\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.model = None\n",
    "        \n",
    "    def preparar_modelo(self):\n",
    "        \"\"\"Inicializa DistilBERT para classifica√ß√£o\"\"\"\n",
    "        self.model = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "            'distilbert-base-uncased',\n",
    "            num_labels=self.num_labels\n",
    "        )\n",
    "        \n",
    "        # Otimizador com learning rate pequeno\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=loss,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def tokenizar_dados(self, exemplos):\n",
    "        \"\"\"Tokeniza textos\"\"\"\n",
    "        return self.tokenizer(\n",
    "            exemplos['text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128  # Reduzir para economizar mem√≥ria\n",
    "        )\n",
    "    \n",
    "    def preparar_dataset(self, dataset_name='imdb'):\n",
    "        \"\"\"\n",
    "        Carrega e prepara dataset\n",
    "        \"\"\"\n",
    "        # Carregar dataset\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        \n",
    "        # Tokenizar\n",
    "        tokenized = dataset.map(\n",
    "            self.tokenizar_dados,\n",
    "            batched=True,\n",
    "            remove_columns=dataset['train'].column_names\n",
    "        )\n",
    "        \n",
    "        # Converter para TF format\n",
    "        train_dataset = tokenized['train'].to_tf_dataset(\n",
    "            columns=['input_ids', 'attention_mask'],\n",
    "            label_cols=['label'],\n",
    "            shuffle=True,\n",
    "            batch_size=16,  # Batch pequeno para M1\n",
    "            collate_fn=DataCollatorWithPadding(\n",
    "                tokenizer=self.tokenizer, \n",
    "                return_tensors='tf'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        test_dataset = tokenized['test'].to_tf_dataset(\n",
    "            columns=['input_ids', 'attention_mask'],\n",
    "            label_cols=['label'],\n",
    "            shuffle=False,\n",
    "            batch_size=16,\n",
    "            collate_fn=DataCollatorWithPadding(\n",
    "                tokenizer=self.tokenizer,\n",
    "                return_tensors='tf'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return train_dataset, test_dataset\n",
    "    \n",
    "    def treinar(self, train_dataset, val_dataset, epochs=3):\n",
    "        \"\"\"\n",
    "        Treina o modelo\n",
    "        \"\"\"\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=2,\n",
    "                restore_best_weights=True\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            train_dataset,\n",
    "            validation_data=val_dataset,\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "\n",
    "# Uso\n",
    "classificador = ClassificadorSentimentos(num_labels=2)\n",
    "classificador.preparar_modelo()\n",
    "train_ds, test_ds = classificador.preparar_dataset('imdb')\n",
    "history = classificador.treinar(train_ds, test_ds, epochs=3)\n",
    "```\n",
    "\n",
    "#### Classifica√ß√£o de Texto com PyTorch\n",
    "\n",
    "```python\n",
    "# classificador_pytorch.py\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TreinadorNLP:\n",
    "    def __init__(self, num_labels=2, device='mps'):\n",
    "        self.device = device\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            'distilbert-base-uncased',\n",
    "            num_labels=num_labels\n",
    "        ).to(device)\n",
    "        \n",
    "    def treinar_epoch(self, dataloader, optimizer, scheduler):\n",
    "        \"\"\"\n",
    "        Treina uma epoch\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        progress = tqdm(dataloader, desc=\"Treino\")\n",
    "        for batch in progress:\n",
    "            # Move para device\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels = batch['labels'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Atualizar progress bar\n",
    "            progress.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "        return total_loss / len(dataloader)\n",
    "    \n",
    "    def avaliar(self, dataloader):\n",
    "        \"\"\"\n",
    "        Avalia o modelo\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc=\"Avalia√ß√£o\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['labels'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                predictions = torch.argmax(outputs.logits, dim=1)\n",
    "                total_correct += (predictions == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "        \n",
    "        accuracy = total_correct / total_samples\n",
    "        return accuracy\n",
    "    \n",
    "    def treinar_completo(self, train_loader, val_loader, epochs=3):\n",
    "        \"\"\"\n",
    "        Pipeline completo\n",
    "        \"\"\"\n",
    "        # Optimizer\n",
    "        optimizer = AdamW(self.model.parameters(), lr=2e-5)\n",
    "        \n",
    "        # Scheduler\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Treino\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nüìö Epoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            train_loss = self.treinar_epoch(train_loader, optimizer, scheduler)\n",
    "            val_accuracy = self.avaliar(val_loader)\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Salvar modelo\n",
    "        self.model.save_pretrained('modelo_nlp_final')\n",
    "        self.tokenizer.save_pretrained('modelo_nlp_final')\n",
    "        \n",
    "        print(\"\\n‚úÖ Modelo salvo em 'modelo_nlp_final/'\")\n",
    "\n",
    "# Uso\n",
    "treinador = TreinadorNLP(num_labels=2, device='mps')\n",
    "treinador.treinar_completo(train_loader, val_loader, epochs=3)\n",
    "```\n",
    "\n",
    "--- \n",
    "\n",
    "### 3.3 Modelos Tabulares\n",
    "\n",
    "Para dados estruturados/tabulares, o M1 √© excelente!\n",
    "\n",
    "#### XGBoost, LightGBM, CatBoost\n",
    "\n",
    "**Por que usar gradient boosting no M1?**\n",
    "- ‚úÖ Extremamente eficientes em mem√≥ria\n",
    "- ‚úÖ N√£o precisam GPU (CPU M1 √© muito r√°pida)\n",
    "- ‚úÖ Performance state-of-the-art em dados tabulares\n",
    "- ‚úÖ Treinam muito r√°pido\n",
    "\n",
    "**Instala√ß√£o:**\n",
    "\n",
    "```bash\n",
    "pip install xgboost lightgbm catboost\n",
    "```\n",
    "\n",
    "**Exemplo completo:**\n",
    "\n",
    "```python\n",
    "# modelos_tabulares.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "class TreinadorTabular:\n",
    "    def __init__(self, X, y):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = \\\n",
    "            train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    def treinar_xgboost(self):\n",
    "        \"\"\"XGBoost - Muito popular\"\"\"\n",
    "        print(\"üöÄ Treinando XGBoost...\")\n",
    "        \n",
    "        model = xgb.XGBClassifier(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            tree_method='hist',  # Mais r√°pido\n",
    "            early_stopping_rounds=50,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            eval_set=[(self.X_test, self.y_test)],\n",
    "            verbose=50\n",
    "        )\n",
    "        \n",
    "        preds = model.predict(self.X_test)\n",
    "        acc = accuracy_score(self.y_test, preds)\n",
    "        print(f\"‚úì XGBoost Accuracy: {acc:.4f}\\n\")\n",
    "        \n",
    "        return model, acc\n",
    "    \n",
    "    def treinar_lightgbm(self):\n",
    "        \"\"\"LightGBM - Mais r√°pido\"\"\"\n",
    "        print(\"‚ö° Treinando LightGBM...\")\n",
    "        \n",
    "        model = lgb.LGBMClassifier(\n",
    "            n_estimators=1000,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            max_depth=6,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            early_stopping_rounds=50\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            eval_set=[(self.X_test, self.y_test)],\n",
    "            verbose=50\n",
    "        )\n",
    "        \n",
    "        preds = model.predict(self.X_test)\n",
    "        acc = accuracy_score(self.y_test, preds)\n",
    "        print(f\"‚úì LightGBM Accuracy: {acc:.4f}\\n\")\n",
    "        \n",
    "        return model, acc\n",
    "    \n",
    "    def treinar_catboost(self):\n",
    "        \"\"\"CatBoost - Melhor para categ√≥ricas\"\"\"\n",
    "        print(\"üê± Treinando CatBoost...\")\n",
    "        \n",
    "        model = CatBoostClassifier(\n",
    "            iterations=1000,\n",
    "            learning_rate=0.05,\n",
    "            depth=6,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=50\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            self.X_train, self.y_train,\n",
    "            eval_set=(self.X_test, self.y_test)\n",
    "        )\n",
    "        \n",
    "        preds = model.predict(self.X_test)\n",
    "        acc = accuracy_score(self.y_test, preds)\n",
    "        print(f\"‚úì CatBoost Accuracy: {acc:.4f}\\n\")\n",
    "        \n",
    "        return model, acc\n",
    "    \n",
    "    def comparar_todos(self):\n",
    "        \"\"\"Compara os 3 modelos\"\"\"\n",
    "        resultados = {}\n",
    "        \n",
    "        resultados['XGBoost'] = self.treinar_xgboost()\n",
    "        resultados['LightGBM'] = self.treinar_lightgbm()\n",
    "        resultados['CatBoost'] = self.treinar_catboost()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"COMPARA√á√ÉO FINAL\")\n",
    "        print(\"=\"*50)\n",
    "        for nome, (modelo, acc) in resultados.items():\n",
    "            print(f\"{nome:12s}: {acc:.4f}\")\n",
    "        \n",
    "        # Melhor modelo\n",
    "        melhor = max(resultados.items(), key=lambda x: x[1][1])\n",
    "        print(f\"\\nüèÜ Melhor modelo: {melhor[0]} ({melhor[1][1]:.4f})\")\n",
    "        \n",
    "        return resultados\n",
    "\n",
    "# Uso com dataset de exemplo\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "treinador = TreinadorTabular(X, y)\n",
    "resultados = treinador.comparar_todos()\n",
    "```\n",
    "\n",
    "#### Redes Neurais para Dados Tabulares\n",
    "\n",
    "```python\n",
    "# nn_tabular.py\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "def criar_nn_tabular(input_dim, num_classes):\n",
    "    \"\"\"\n",
    "    Rede neural simples para dados tabulares\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Input(shape=(input_dim,)),\n",
    "        \n",
    "        # BatchNorm ajuda muito em tabulares\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        \n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        \n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        \n",
    "        keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Treinar\n",
    "model = criar_nn_tabular(input_dim=20, num_classes=2)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "--- \n",
    "\n",
    "### üí° Dicas de Performance no M1\n",
    "\n",
    "**Para Imagens:**\n",
    "- Use MobileNet/EfficientNet como base\n",
    "- Batch size 32-64 √© ideal\n",
    "- Mixed precision economiza ~40% mem√≥ria\n",
    "\n",
    "**Para NLP:**\n",
    "- DistilBERT > BERT para M1 16GB\n",
    "- max_length=128 em vez de 512\n",
    "- Batch size 16-32\n",
    "\n",
    "**Para Tabulares:**\n",
    "- LightGBM geralmente √© o mais r√°pido no M1\n",
    "- Aproveita CPU multicores muito bem\n",
    "- N√£o precisa GPU!\n",
    "\n",
    "--- \n",
    "\n",
    "### ‚úÖ Checklist M√≥dulo 3\n",
    "\n",
    "- [ ] Treinei classificador de imagens com transfer learning\n",
    "- [ ] Entendo diferen√ßa entre fase 1 (frozen) e fase 2 (fine-tuning)\n",
    "- [ ] Consigo treinar modelo NLP com DistilBERT\n",
    "- [ ] Sei usar XGBoost/LightGBM/CatBoost\n",
    "- [ ] Mixed precision ativado nos modelos\n",
    "- [ ] Callbacks implementados (Early Stopping, ReduceLR)\n",
    "\n",
    "--- \n",
    "\n",
    "### üéØ Pr√≥ximos Passos\n",
    "\n",
    "No **M√≥dulo 4**, vamos aprender t√©cnicas avan√ßadas: quantiza√ß√£o, pruning, e como comprimir modelos para ficarem ainda mais eficientes!\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
