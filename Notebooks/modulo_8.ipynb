{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M\u00f3dulo 8: Boas Pr\u00e1ticas e Troubleshooting\n",
    "\n",
    "## \ud83d\udccb \u00cdndice\n",
    "\n",
    "### 8.1 Workflows Eficientes\n",
    "- Organiza\u00e7\u00e3o de projectos\n",
    "- Versionamento de modelos\n",
    "- Experimenta\u00e7\u00e3o r\u00e1pida\n",
    "- Reprodutibilidade\n",
    "\n",
    "### 8.2 Problemas Comuns\n",
    "- Out of Memory (OOM)\n",
    "- Treino lento\n",
    "- Overfitting/Underfitting\n",
    "- Incompatibilidades\n",
    "\n",
    "### 8.3 Quando Usar Cloud Computing\n",
    "- Limita\u00e7\u00f5es do M1 16GB\n",
    "- Alternativas cloud\n",
    "- Estrat\u00e9gia h\u00edbrida\n",
    "\n",
    "---\n",
    "\n",
    "## 8.1 Workflows Eficientes\n",
    "\n",
    "### Organiza\u00e7\u00e3o de Projectos\n",
    "\n",
    "```\n",
    "projeto_ml/\n",
    "\u251c\u2500\u2500 data/\n",
    "\u2502   \u251c\u2500\u2500 raw/              # Dados originais (nunca modificar!)\n",
    "\u2502   \u251c\u2500\u2500 processed/        # Dados processados\n",
    "\u2502   \u2514\u2500\u2500 splits/           # train/val/test\n",
    "\u251c\u2500\u2500 notebooks/            # Explora\u00e7\u00e3o e an\u00e1lise\n",
    "\u2502   \u251c\u2500\u2500 01_exploracao.ipynb\n",
    "\u2502   \u2514\u2500\u2500 02_experimentos.ipynb\n",
    "\u251c\u2500\u2500 src/                  # C\u00f3digo de produ\u00e7\u00e3o\n",
    "\u2502   \u251c\u2500\u2500 data/\n",
    "\u2502   \u2502   \u251c\u2500\u2500 loader.py\n",
    "\u2502   \u2502   \u2514\u2500\u2500 preprocessor.py\n",
    "\u2502   \u251c\u2500\u2500 models/\n",
    "\u2502   \u2502   \u251c\u2500\u2500 base_model.py\n",
    "\u2502   \u2502   \u2514\u2500\u2500 custom_model.py\n",
    "\u2502   \u251c\u2500\u2500 training/\n",
    "\u2502   \u2502   \u251c\u2500\u2500 trainer.py\n",
    "\u2502   \u2502   \u2514\u2500\u2500 callbacks.py\n",
    "\u2502   \u2514\u2500\u2500 utils/\n",
    "\u2502       \u2514\u2500\u2500 helpers.py\n",
    "\u251c\u2500\u2500 models/               # Modelos treinados\n",
    "\u2502   \u251c\u2500\u2500 checkpoints/\n",
    "\u2502   \u2514\u2500\u2500 final/\n",
    "\u251c\u2500\u2500 logs/                 # TensorBoard, logs\n",
    "\u251c\u2500\u2500 configs/              # Configura\u00e7\u00f5es\n",
    "\u2502   \u2514\u2500\u2500 config.yaml\n",
    "\u251c\u2500\u2500 tests/                # Testes unit\u00e1rios\n",
    "\u251c\u2500\u2500 requirements.txt\n",
    "\u251c\u2500\u2500 README.md\n",
    "\u2514\u2500\u2500 .gitignore\n",
    "```\n",
    "\n",
    "**Regras de ouro:**\n",
    "- \u2705 Dados raw nunca s\u00e3o modificados\n",
    "- \u2705 C\u00f3digo em `src/`, explora\u00e7\u00e3o em `notebooks/`\n",
    "- \u2705 Configura\u00e7\u00f5es em ficheiros separados\n",
    "- \u2705 `.gitignore` para modelos grandes\n",
    "\n",
    "### Template .gitignore\n",
    "\n",
    "```bash\n",
    "# .gitignore\n",
    "# Dados\n",
    "data/raw/*\n",
    "data/processed/*\n",
    "*.csv\n",
    "*.json\n",
    "*.pkl\n",
    "\n",
    "# Modelos (>100MB)\n",
    "models/*.keras\n",
    "models/*.h5\n",
    "models/*.pth\n",
    "*.ckpt\n",
    "*.npz\n",
    "\n",
    "# Logs\n",
    "logs/*\n",
    "*.log\n",
    "\n",
    "# Python\n",
    "__pycache__/\n",
    "*.pyc\n",
    ".ipynb_checkpoints/\n",
    "\n",
    "# MacOS\n",
    ".DS_Store\n",
    "\n",
    "# Ambientes\n",
    "venv/\n",
    ".conda/\n",
    "```\n",
    "\n",
    "### Versionamento de Modelos\n",
    "\n",
    "```python\n",
    "# model_versioning.py\n",
    "\"\"\"\n",
    "Sistema simples de versionamento de modelos\n",
    "\"\"\"\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class ModelVersioner:\n",
    "    \"\"\"Gere vers\u00f5es de modelos com metadata\"\"\"\n",
    "    \n",
    "    def __init__(self, base_dir=\"models\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "        self.metadata_file = self.base_dir / \"versions.json\"\n",
    "        \n",
    "        # Carregar hist\u00f3rico\n",
    "        if self.metadata_file.exists():\n",
    "            with open(self.metadata_file) as f:\n",
    "                self.versions = json.load(f)\n",
    "        else:\n",
    "            self.versions = {}\n",
    "    \n",
    "    def save_model(self, model, name, metrics, hyperparams, notes=\"\"):\n",
    "        \"\"\"\n",
    "        Guarda modelo com metadata completa\n",
    "        \n",
    "        Args:\n",
    "            model: Modelo treinado\n",
    "            name: Nome base (ex: \"classificador_caes\")\n",
    "            metrics: Dict com m\u00e9tricas (accuracy, loss, etc.)\n",
    "            hyperparams: Dict com hiperpar\u00e2metros\n",
    "            notes: Notas adicionais\n",
    "        \"\"\"\n",
    "        # Gerar vers\u00e3o \u00fanica\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        version_id = f\"{name}_v{timestamp}\"\n",
    "        \n",
    "        # Guardar modelo\n",
    "        model_path = self.base_dir / f\"{version_id}.keras\"\n",
    "        model.save(model_path)\n",
    "        \n",
    "        # Metadata\n",
    "        self.versions[version_id] = {\n",
    "            \"name\": name,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"path\": str(model_path),\n",
    "            \"metrics\": metrics,\n",
    "            \"hyperparams\": hyperparams,\n",
    "            \"notes\": notes,\n",
    "            \"size_mb\": model_path.stat().st_size / (1024**2)\n",
    "        }\n",
    "        \n",
    "        # Guardar metadata\n",
    "        with open(self.metadata_file, 'w') as f:\n",
    "            json.dump(self.versions, f, indent=2)\n",
    "        \n",
    "        print(f\"\u2713 Modelo guardado: {version_id}\")\n",
    "        print(f\"  Accuracy: {metrics.get('accuracy', 'N/A')}\")\n",
    "        print(f\"  Tamanho: {self.versions[version_id]['size_mb']:.1f}MB\")\n",
    "        \n",
    "        return version_id\n",
    "    \n",
    "    def list_versions(self, name=None):\n",
    "        \"\"\"Lista todas as vers\u00f5es (ou filtrado por nome)\"\"\"\n",
    "        print(\"\\n\ud83d\udcca VERS\u00d5ES DE MODELOS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        versions = self.versions.items()\n",
    "        if name:\n",
    "            versions = [(k,v) for k,v in versions if v['name'] == name]\n",
    "        \n",
    "        for version_id, info in sorted(versions, key=lambda x: x[1]['timestamp'], reverse=True):\n",
    "            print(f\"\\n{version_id}\")\n",
    "            print(f\"  Data: {info['timestamp']}\")\n",
    "            print(f\"  Accuracy: {info['metrics'].get('accuracy', 'N/A'):.4f}\")\n",
    "            print(f\"  Tamanho: {info['size_mb']:.1f}MB\")\n",
    "            if info['notes']:\n",
    "                print(f\"  Notas: {info['notes']}\")\n",
    "    \n",
    "    def load_best(self, name, metric='accuracy'):\n",
    "        \"\"\"Carrega melhor modelo baseado numa m\u00e9trica\"\"\"\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        candidates = [(k,v) for k,v in self.versions.items() if v['name'] == name]\n",
    "        if not candidates:\n",
    "            raise ValueError(f\"Nenhum modelo encontrado com nome: {name}\")\n",
    "        \n",
    "        best = max(candidates, key=lambda x: x[1]['metrics'].get(metric, 0))\n",
    "        best_id, best_info = best\n",
    "        \n",
    "        print(f\"\ud83d\udce5 Carregando melhor modelo: {best_id}\")\n",
    "        print(f\"  {metric}: {best_info['metrics'][metric]:.4f}\")\n",
    "        \n",
    "        return tf.keras.models.load_model(best_info['path'])\n",
    "\n",
    "# Uso\n",
    "versioner = ModelVersioner()\n",
    "\n",
    "# Ap\u00f3s treinar\n",
    "versioner.save_model(\n",
    "    model=model,\n",
    "    name=\"classificador_caes\",\n",
    "    metrics={'accuracy': 0.89, 'val_accuracy': 0.85, 'loss': 0.31},\n",
    "    hyperparams={'lr': 1e-3, 'batch_size': 32, 'epochs': 30},\n",
    "    notes=\"Transfer learning + fine-tuning, dataset balanceado\"\n",
    ")\n",
    "\n",
    "# Listar\n",
    "versioner.list_versions(\"classificador_caes\")\n",
    "\n",
    "# Carregar melhor\n",
    "best_model = versioner.load_best(\"classificador_caes\", metric='val_accuracy')\n",
    "```\n",
    "\n",
    "### Configura\u00e7\u00f5es Centralizadas\n",
    "\n",
    "```yaml\n",
    "# configs/config.yaml\n",
    "# Centraliza todos os hiperpar\u00e2metros\n",
    "\n",
    "project:\n",
    "  name: \"classificador_caes\"\n",
    "  seed: 42\n",
    "  \n",
    "data:\n",
    "  path: \"dataset_caes\"\n",
    "  img_size: [224, 224]\n",
    "  batch_size: 32\n",
    "  val_split: 0.2\n",
    "  \n",
    "  augmentation:\n",
    "    horizontal_flip: true\n",
    "    rotation: 0.2\n",
    "    zoom: 0.2\n",
    "    brightness: 0.1\n",
    "\n",
    "model:\n",
    "  architecture: \"efficientnetb0\"\n",
    "  pretrained: true\n",
    "  num_classes: 10\n",
    "  dropout: 0.3\n",
    "  dense_units: 256\n",
    "\n",
    "training:\n",
    "  epochs: 50\n",
    "  learning_rate: 0.001\n",
    "  optimizer: \"adam\"\n",
    "  loss: \"sparse_categorical_crossentropy\"\n",
    "  \n",
    "  callbacks:\n",
    "    early_stopping:\n",
    "      patience: 10\n",
    "      monitor: \"val_accuracy\"\n",
    "    \n",
    "    reduce_lr:\n",
    "      factor: 0.5\n",
    "      patience: 5\n",
    "      min_lr: 1.0e-7\n",
    "\n",
    "hardware:\n",
    "  mixed_precision: true\n",
    "  gpu_memory_limit: 12  # GB\n",
    "```\n",
    "\n",
    "```python\n",
    "# config_loader.py\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Carrega e valida configura\u00e7\u00f5es\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path=\"configs/config.yaml\"):\n",
    "        with open(config_path) as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "        \n",
    "        self.validate()\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Valida configura\u00e7\u00f5es\"\"\"\n",
    "        # Verificar campos obrigat\u00f3rios\n",
    "        required = ['project', 'data', 'model', 'training']\n",
    "        for field in required:\n",
    "            if field not in self.config:\n",
    "                raise ValueError(f\"Campo obrigat\u00f3rio ausente: {field}\")\n",
    "        \n",
    "        # Validar valores\n",
    "        if self.config['data']['batch_size'] > 128:\n",
    "            print(\"\u26a0\ufe0f  batch_size >128 pode causar OOM no M1\")\n",
    "        \n",
    "        if self.config['training']['learning_rate'] > 0.01:\n",
    "            print(\"\u26a0\ufe0f  Learning rate muito alto!\")\n",
    "    \n",
    "    def get(self, path, default=None):\n",
    "        \"\"\"Acede valores com dot notation: config.get('data.batch_size')\"\"\"\n",
    "        keys = path.split('.')\n",
    "        value = self.config\n",
    "        \n",
    "        for key in keys:\n",
    "            if isinstance(value, dict) and key in value:\n",
    "                value = value[key]\n",
    "            else:\n",
    "                return default\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return self.config[key]\n",
    "\n",
    "# Uso\n",
    "config = Config()\n",
    "batch_size = config.get('data.batch_size')\n",
    "lr = config.get('training.learning_rate')\n",
    "```\n",
    "\n",
    "### Experimenta\u00e7\u00e3o R\u00e1pida\n",
    "\n",
    "```python\n",
    "# experiment_tracker.py\n",
    "\"\"\"\n",
    "Sistema leve para tracking de experimentos\n",
    "Alternativa simples ao W&B quando offline\n",
    "\"\"\"\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "class ExperimentTracker:\n",
    "    \"\"\"Regista experimentos localmente\"\"\"\n",
    "    \n",
    "    def __init__(self, project_name, log_dir=\"experiments\"):\n",
    "        self.project_name = project_name\n",
    "        self.log_dir = Path(log_dir) / project_name\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Criar experimento\n",
    "        self.exp_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.exp_dir = self.log_dir / self.exp_id\n",
    "        self.exp_dir.mkdir()\n",
    "        \n",
    "        self.metrics = []\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        print(f\"\ud83e\uddea Experimento iniciado: {self.exp_id}\")\n",
    "    \n",
    "    def log_params(self, params):\n",
    "        \"\"\"Regista hiperpar\u00e2metros\"\"\"\n",
    "        with open(self.exp_dir / \"params.json\", 'w') as f:\n",
    "            json.dump(params, f, indent=2)\n",
    "    \n",
    "    def log_metric(self, name, value, step=None):\n",
    "        \"\"\"Regista m\u00e9trica\"\"\"\n",
    "        self.metrics.append({\n",
    "            'name': name,\n",
    "            'value': value,\n",
    "            'step': step,\n",
    "            'timestamp': time.time() - self.start_time\n",
    "        })\n",
    "    \n",
    "    def log_metrics(self, metrics_dict, step=None):\n",
    "        \"\"\"Regista m\u00faltiplas m\u00e9tricas\"\"\"\n",
    "        for name, value in metrics_dict.items():\n",
    "            self.log_metric(name, value, step)\n",
    "    \n",
    "    def save_model(self, model, name=\"model.keras\"):\n",
    "        \"\"\"Guarda modelo no experimento\"\"\"\n",
    "        model_path = self.exp_dir / name\n",
    "        model.save(model_path)\n",
    "        return model_path\n",
    "    \n",
    "    def finish(self, status=\"completed\", notes=\"\"):\n",
    "        \"\"\"Finaliza experimento\"\"\"\n",
    "        duration = time.time() - self.start_time\n",
    "        \n",
    "        summary = {\n",
    "            'exp_id': self.exp_id,\n",
    "            'status': status,\n",
    "            'duration_seconds': duration,\n",
    "            'notes': notes,\n",
    "            'final_metrics': {\n",
    "                name: value \n",
    "                for name, value, *_ in \n",
    "                [m.values() for m in self.metrics[-10:]]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(self.exp_dir / \"summary.json\", 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        with open(self.exp_dir / \"metrics.json\", 'w') as f:\n",
    "            json.dump(self.metrics, f, indent=2)\n",
    "        \n",
    "        print(f\"\u2713 Experimento conclu\u00eddo: {self.exp_id}\")\n",
    "        print(f\"  Dura\u00e7\u00e3o: {duration/60:.1f} min\")\n",
    "\n",
    "# Uso no treino\n",
    "tracker = ExperimentTracker(\"classificador_caes\")\n",
    "\n",
    "# Registar hiperpar\u00e2metros\n",
    "tracker.log_params({\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 50,\n",
    "    'architecture': 'efficientnetb0'\n",
    "})\n",
    "\n",
    "# Durante treino\n",
    "for epoch in range(epochs):\n",
    "    # ... treino ...\n",
    "    \n",
    "    tracker.log_metrics({\n",
    "        'loss': train_loss,\n",
    "        'accuracy': train_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': val_acc\n",
    "    }, step=epoch)\n",
    "\n",
    "# No fim\n",
    "tracker.save_model(model)\n",
    "tracker.finish(status=\"completed\", notes=\"Melhor modelo at\u00e9 agora!\")\n",
    "```\n",
    "\n",
    "### Reprodutibilidade\n",
    "\n",
    "```python\n",
    "# reproducibility.py\n",
    "\"\"\"\n",
    "Garante reprodutibilidade de experimentos\n",
    "\"\"\"\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Define seed global para reprodutibilidade\n",
    "    \n",
    "    \u26a0\ufe0f IMPORTANTE: Chama isto ANTES de qualquer treino!\n",
    "    \"\"\"\n",
    "    # Python\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # TensorFlow\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "    # Para opera\u00e7\u00f5es determ\u00ednicas (mais lento mas reprodut\u00edvel)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    print(f\"\u2713 Seed global definido: {seed}\")\n",
    "    print(\"\u26a0\ufe0f  Treino ser\u00e1 determin\u00edstico (mais lento)\")\n",
    "\n",
    "def get_system_info():\n",
    "    \"\"\"Regista informa\u00e7\u00e3o do sistema para debug\"\"\"\n",
    "    import platform\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    info = {\n",
    "        'python_version': platform.python_version(),\n",
    "        'platform': platform.platform(),\n",
    "        'processor': platform.processor(),\n",
    "        'tensorflow_version': tf.__version__,\n",
    "        'gpu_available': len(tf.config.list_physical_devices('GPU')) > 0,\n",
    "        'mixed_precision': tf.keras.mixed_precision.global_policy().name\n",
    "    }\n",
    "    \n",
    "    return info\n",
    "\n",
    "# Uso no in\u00edcio do script\n",
    "set_seed(42)\n",
    "system_info = get_system_info()\n",
    "print(json.dumps(system_info, indent=2))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8.2 Problemas Comuns\n",
    "\n",
    "### Out of Memory (OOM)\n",
    "\n",
    "**Sintomas:**\n",
    "- Kernel crashes\n",
    "- Mensagem \"ResourceExhaustedError\"\n",
    "- Sistema congela\n",
    "\n",
    "**Diagn\u00f3stico:**\n",
    "```python\n",
    "# check_memory.py\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "\n",
    "def check_memory_usage():\n",
    "    \"\"\"Verifica uso actual de mem\u00f3ria\"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    \n",
    "    print(\"\ud83d\udcbe MEM\u00d3RIA DO SISTEMA\")\n",
    "    print(f\"Total:      {mem.total / 1e9:.1f} GB\")\n",
    "    print(f\"Dispon\u00edvel: {mem.available / 1e9:.1f} GB\")\n",
    "    print(f\"Usada:      {mem.used / 1e9:.1f} GB ({mem.percent}%)\")\n",
    "    \n",
    "    if mem.available < 8e9:  # <8GB\n",
    "        print(\"\u26a0\ufe0f  ALERTA: Pouca mem\u00f3ria dispon\u00edvel!\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Durante treino\n",
    "class MemoryCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"Monitoriza mem\u00f3ria durante treino\"\"\"\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        mem = psutil.virtual_memory()\n",
    "        print(f\"\\nMem\u00f3ria ap\u00f3s epoch {epoch}: {mem.percent}%\")\n",
    "        \n",
    "        if mem.percent > 90:\n",
    "            print(\"\u26a0\ufe0f  Mem\u00f3ria >90%! Considera parar treino\")\n",
    "```\n",
    "\n",
    "**Solu\u00e7\u00f5es (em ordem de prioridade):**\n",
    "\n",
    "1. **Reduzir batch_size**\n",
    "```python\n",
    "# De 32 para 16\n",
    "batch_size = 16  \n",
    "\n",
    "# Ou usar gradient accumulation para simular batch maior\n",
    "def train_with_grad_accum(model, data, accum_steps=4):\n",
    "    optimizer.zero_grad()\n",
    "    for i, batch in enumerate(data):\n",
    "        loss = model(batch) / accum_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if (i + 1) % accum_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "2. **Mixed Precision**\n",
    "```python\n",
    "# Economiza ~50% RAM\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "```\n",
    "\n",
    "3. **Limpar cache**\n",
    "```python\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "\n",
    "# Ap\u00f3s cada epoch\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "```\n",
    "\n",
    "4. **Reduzir tamanho do modelo**\n",
    "```python\n",
    "# Usar modelo mais leve\n",
    "model = tf.keras.applications.MobileNetV2()  # em vez de ResNet50\n",
    "```\n",
    "\n",
    "### Treino Lento\n",
    "\n",
    "**Diagn\u00f3stico:**\n",
    "```python\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "def benchmark_pipeline(dataset, num_steps=100):\n",
    "    \"\"\"Mede velocidade do pipeline de dados\"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    for i, batch in enumerate(dataset.take(num_steps)):\n",
    "        if i == 0:\n",
    "            first_batch_time = time.time() - start\n",
    "        pass\n",
    "    total_time = time.time() - start\n",
    "    \n",
    "    print(f\"\u23f1\ufe0f  BENCHMARK\")\n",
    "    print(f\"Primeiro batch: {first_batch_time:.2f}s (compila grafo)\")\n",
    "    print(f\"Total {num_steps} batches: {total_time:.2f}s\")\n",
    "    print(f\"M\u00e9dia por batch: {total_time/num_steps:.3f}s\")\n",
    "    print(f\"Batches/segundo: {num_steps/total_time:.1f}\")\n",
    "```\n",
    "\n",
    "**Solu\u00e7\u00f5es:**\n",
    "\n",
    "1. **Prefetch e Cache**\n",
    "```python\n",
    "# Carrega pr\u00f3ximo batch enquanto treina\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Cache dataset pequenos na RAM\n",
    "dataset = dataset.cache()\n",
    "```\n",
    "\n",
    "2. **Verificar GPU**\n",
    "```python\n",
    "# GPU deve estar activa\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Se vazio, reinstala Metal\n",
    "# pip uninstall tensorflow-metal\n",
    "# pip install tensorflow-metal==1.1.0\n",
    "```\n",
    "\n",
    "3. **Optimizar I/O**\n",
    "```python\n",
    "# Paralelizar carregamento\n",
    "dataset = dataset.map(\n",
    "    preprocess_fn,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "# Reduzir tamanho de imagens se poss\u00edvel\n",
    "img_size = (224, 224)  # em vez de (512, 512)\n",
    "```\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "**Sintomas:**\n",
    "- Train accuracy alta, val accuracy baixa\n",
    "- Gap >10% entre train e validation\n",
    "\n",
    "**Diagn\u00f3stico:**\n",
    "```python\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Visualiza overfitting\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Train')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation')\n",
    "    ax1.set_title('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Loss  \n",
    "    ax2.plot(history.history['loss'], label='Train')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation')\n",
    "    ax2.set_title('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curve.png')\n",
    "    \n",
    "    # Detectar overfitting\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    gap = final_train_acc - final_val_acc\n",
    "    \n",
    "    if gap > 0.1:\n",
    "        print(f\"\u26a0\ufe0f  OVERFITTING DETECTADO!\")\n",
    "        print(f\"   Gap train-val: {gap:.2%}\")\n",
    "```\n",
    "\n",
    "**Solu\u00e7\u00f5es:**\n",
    "\n",
    "1. **Mais dados / Data Augmentation**\n",
    "```python\n",
    "augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.3),      # Aumenta\n",
    "    tf.keras.layers.RandomZoom(0.3),          # Aumenta\n",
    "    tf.keras.layers.RandomTranslation(0.2, 0.2),\n",
    "    tf.keras.layers.RandomContrast(0.2),\n",
    "])\n",
    "```\n",
    "\n",
    "2. **Regulariza\u00e7\u00e3o**\n",
    "```python\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.Dropout(0.5),  # Aumenta dropout\n",
    "    tf.keras.layers.Dense(256, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(num_classes)\n",
    "])\n",
    "```\n",
    "\n",
    "3. **Early Stopping**\n",
    "```python\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,  # Reduz patience\n",
    "    restore_best_weights=True\n",
    ")\n",
    "```\n",
    "\n",
    "### Underfitting\n",
    "\n",
    "**Sintomas:**\n",
    "- Train e val accuracy ambas baixas\n",
    "- Loss n\u00e3o desce\n",
    "\n",
    "**Solu\u00e7\u00f5es:**\n",
    "\n",
    "1. **Modelo maior**\n",
    "```python\n",
    "# Mais camadas/neur\u00f3nios\n",
    "dense_units = 512  # era 256\n",
    "```\n",
    "\n",
    "2. **Learning rate**\n",
    "```python\n",
    "# Testar LRs diferentes\n",
    "for lr in [1e-5, 1e-4, 1e-3, 1e-2]:\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr))\n",
    "    # treinar...\n",
    "```\n",
    "\n",
    "3. **Mais epochs**\n",
    "```python\n",
    "epochs = 100  # em vez de 30\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8.3 Quando Usar Cloud Computing\n",
    "\n",
    "### Limita\u00e7\u00f5es do M1 16GB\n",
    "\n",
    "| Tarefa | M1 16GB | Recomenda\u00e7\u00e3o |\n",
    "|--------|---------|--------------|\n",
    "| **Modelos <1B params** | \u2705 Excelente | Local |\n",
    "| **Fine-tuning 7B (LoRA)** | \u2705 Vi\u00e1vel | Local |\n",
    "| **Fine-tuning 13B+** | \u26a0\ufe0f Dif\u00edcil | Cloud |\n",
    "| **Treino from scratch LLM** | \u274c Imposs\u00edvel | Cloud |\n",
    "| **Datasets >50GB** | \u274c Lento | Cloud |\n",
    "| **Batch processing grande** | \u26a0\ufe0f Lento | Cloud |\n",
    "\n",
    "### Alternativas Cloud\n",
    "\n",
    "**Google Colab (Gratuito/Pro)**\n",
    "```python\n",
    "# Vantagens:\n",
    "# - GPU T4 gratuita\n",
    "# - 12GB RAM (gratuito), 25GB (Pro)\n",
    "# - Setup zero\n",
    "\n",
    "# Desvantagens:\n",
    "# - Sess\u00f5es limitadas (12h gratuito)\n",
    "# - Pode desligar aleatoriamente\n",
    "# - Dados perdidos se n\u00e3o guardar\n",
    "\n",
    "# Quando usar: Experimenta\u00e7\u00e3o r\u00e1pida, tutoriais\n",
    "```\n",
    "\n",
    "**Kaggle Notebooks (Gratuito)**\n",
    "```python\n",
    "# Vantagens:\n",
    "# - GPU P100/T4 30h/semana\n",
    "# - 16GB RAM\n",
    "# - Datasets p\u00fablicos integrados\n",
    "\n",
    "# Desvantagens:\n",
    "# - 9h por sess\u00e3o\n",
    "# - Internet limitada\n",
    "\n",
    "# Quando usar: Competi\u00e7\u00f5es, datasets p\u00fablicos\n",
    "```\n",
    "\n",
    "**AWS/GCP/Azure**\n",
    "```python\n",
    "# Vantagens:\n",
    "# - GPUs potentes (A100, V100)\n",
    "# - Escal\u00e1vel\n",
    "# - Controlo total\n",
    "\n",
    "# Desvantagens:\n",
    "# - Pago (caro!)\n",
    "# - Setup complexo\n",
    "\n",
    "# Quando usar: Produ\u00e7\u00e3o, modelos grandes\n",
    "```\n",
    "\n",
    "### Estrat\u00e9gia H\u00edbrida\n",
    "\n",
    "**Desenvolvimento local + Treino cloud:**\n",
    "\n",
    "```python\n",
    "# hybrid_workflow.py\n",
    "\"\"\"\n",
    "Desenvolve local, treina na cloud\n",
    "\"\"\"\n",
    "\n",
    "# 1. Desenvolver e testar local (M1)\n",
    "# - Usa subset pequeno (10% dados)\n",
    "# - Testa pipeline completo\n",
    "# - Debugging\n",
    "\n",
    "# 2. Quando pronto, enviar para cloud\n",
    "# - C\u00f3digo versionado (git)\n",
    "# - Configura\u00e7\u00f5es em YAML\n",
    "# - Scripts automatizados\n",
    "\n",
    "# 3. Treinar na cloud\n",
    "# - Dataset completo\n",
    "# - M\u00faltiplos GPUs se necess\u00e1rio\n",
    "# - Checkpoints para S3/GCS\n",
    "\n",
    "# 4. Download modelo final para M1\n",
    "# - Infer\u00eancia local\n",
    "# - Deployment\n",
    "```\n",
    "\n",
    "**Script para sync:**\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# sync_to_cloud.sh\n",
    "\n",
    "# Upload c\u00f3digo\n",
    "rsync -avz --exclude='data/' --exclude='models/' \\\n",
    "    ./ user@cloud-instance:/home/user/projeto/\n",
    "\n",
    "# Upload dados (se pequenos)\n",
    "rsync -avz data/processed/ \\\n",
    "    user@cloud-instance:/home/user/projeto/data/\n",
    "\n",
    "# Executar treino remoto\n",
    "ssh user@cloud-instance \\\n",
    "    \"cd /home/user/projeto && python train.py --config cloud_config.yaml\"\n",
    "\n",
    "# Download modelo treinado\n",
    "rsync -avz user@cloud-instance:/home/user/projeto/models/final/ \\\n",
    "    ./models/final/\n",
    "```\n",
    "\n",
    "### Decis\u00e3o: Local vs Cloud\n",
    "\n",
    "**Usa M1 16GB quando:**\n",
    "- \u2705 Modelo <7B par\u00e2metros\n",
    "- \u2705 Dataset <20GB\n",
    "- \u2705 Fine-tuning com LoRA\n",
    "- \u2705 Prototipagem\n",
    "- \u2705 Infer\u00eancia\n",
    "- \u2705 Desenvolvimento\n",
    "\n",
    "**Usa Cloud quando:**\n",
    "- \u2705 Modelo >13B par\u00e2metros\n",
    "- \u2705 Dataset >50GB\n",
    "- \u2705 Treino from scratch\n",
    "- \u2705 M\u00faltiplas experi\u00eancias paralelas\n",
    "- \u2705 Deadline apertado\n",
    "- \u2705 Produ\u00e7\u00e3o de alta escala\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udccb Checklist Final de Boas Pr\u00e1ticas\n",
    "\n",
    "### Antes de Come\u00e7ar\n",
    "- [ ] Ambiente virtual activado\n",
    "- [ ] Configura\u00e7\u00f5es centralizadas (YAML)\n",
    "- [ ] .gitignore configurado\n",
    "- [ ] Seed definido para reprodutibilidade\n",
    "\n",
    "### Durante Desenvolvimento\n",
    "- [ ] C\u00f3digo organizado (src/ vs notebooks/)\n",
    "- [ ] Testes com subset pequeno primeiro\n",
    "- [ ] Versionamento de modelos activo\n",
    "- [ ] Tracking de experimentos\n",
    "\n",
    "### Durante Treino\n",
    "- [ ] Mixed precision activada\n",
    "- [ ] Callbacks configurados (Early Stop, ReduceLR)\n",
    "- [ ] Monitoriza\u00e7\u00e3o de recursos\n",
    "- [ ] Checkpoints a guardar\n",
    "\n",
    "### Ap\u00f3s Treino\n",
    "- [ ] Modelo versionado com metadata\n",
    "- [ ] M\u00e9tricas documentadas\n",
    "- [ ] Curvas de treino analisadas\n",
    "- [ ] README actualizado\n",
    "\n",
    "### Troubleshooting\n",
    "- [ ] Logs guardados\n",
    "- [ ] Configura\u00e7\u00f5es documentadas\n",
    "- [ ] Testes de reprodutibilidade\n",
    "- [ ] Plano B (cloud) se necess\u00e1rio\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Resumo\n",
    "\n",
    "Dominas agora:\n",
    "- \u2705 Workflows profissionais\n",
    "- \u2705 Versionamento de modelos\n",
    "- \u2705 Troubleshooting sistem\u00e1tico\n",
    "- \u2705 Quando escalar para cloud\n",
    "\n",
    "**Pr\u00f3ximos passos:**\n",
    "1. Aplica estas pr\u00e1ticas nos teus projectos\n",
    "2. Cria templates reutiliz\u00e1veis\n",
    "3. Automatiza processos repetitivos\n",
    "4. Documenta aprendizagens\n",
    "\n",
    "**Lembra-te:**\n",
    "> \"Horas de debugging podem poupar minutos de planeamento\" \ud83d\ude43"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}