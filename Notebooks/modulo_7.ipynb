{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M\u00f3dulo 7: Projectos Pr\u00e1ticos para M1 16GB\n",
    "\n",
    "## \ud83d\udccb Vis\u00e3o Geral\n",
    "\n",
    "| Projecto | Tempo | Dificuldade | Mem\u00f3ria | Dataset M\u00edn. |\n",
    "|----------|-------|-------------|---------|--------------|\n",
    "| **1. Classificador Imagens** | 1-1.5h | \u2b50\u2b50\u2b50 | 6-8GB | 50/classe |\n",
    "| **2. An\u00e1lise Sentimentos** | 35-40min | \u2b50\u2b50\u2b50\u2b50 | 8-10GB | 300 total |\n",
    "| **3. Fine-tuning LLM** | 1.5-2h | \u2b50\u2b50\u2b50\u2b50\u2b50 | 6-7GB | 100 exemplos |\n",
    "\n",
    "---\n",
    "\n",
    "## \u26a0\ufe0f Prepara\u00e7\u00e3o Obrigat\u00f3ria (Todos os Projectos)\n",
    "\n",
    "### Verifica\u00e7\u00f5es Essenciais\n",
    "\n",
    "```bash\n",
    "# 1. Espa\u00e7o em disco (m\u00edn. 20GB)\n",
    "df -h\n",
    "\n",
    "# 2. Python ARM64 (CR\u00cdTICO!)\n",
    "python -c \"import platform; print(platform.machine())\"\n",
    "# Deve retornar: arm64\n",
    "\n",
    "# 3. Ambiente correcto\n",
    "conda activate ml-m1\n",
    "\n",
    "# 4. Mem\u00f3ria dispon\u00edvel (m\u00edn. 8GB)\n",
    "vm_stat | grep \"Pages free\"\n",
    "```\n",
    "\n",
    "### Regras de Ouro\n",
    "\n",
    "\u2705 **SEMPRE:**\n",
    "- Fecha Chrome, Slack e apps pesadas\n",
    "- Verifica dataset antes de treinar\n",
    "- Usa mixed precision (FP16)\n",
    "- Guarda checkpoints regularmente\n",
    "\n",
    "\u274c **NUNCA:**\n",
    "- Interrompas treino sem motivo\n",
    "- Uses Python x86 (s\u00f3 ARM64!)\n",
    "- Treines com batch_size >64\n",
    "- Ignores avisos de mem\u00f3ria\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\uddbc\ufe0f Projecto 1: Classificador de Imagens\n",
    "\n",
    "### Objectivo\n",
    "Classificar 10 ra\u00e7as de c\u00e3es portugueses com 85%+ accuracy.\n",
    "\n",
    "### Requisitos Espec\u00edficos\n",
    "- M\u00ednimo 50 imagens/classe (ideal: 100+)\n",
    "- Formatos: JPG ou PNG\n",
    "- Nomes sem espa\u00e7os: `cao_da_serra_estrela`\n",
    "\n",
    "### Script 1: Organizar Dataset\n",
    "\n",
    "```python\n",
    "# 1_organizar_dataset.py\n",
    "from pathlib import Path\n",
    "\n",
    "class OrganizadorDataset:\n",
    "    \"\"\"Cria estrutura: dataset/train/classe/ e dataset/validation/classe/\"\"\"\n",
    "    \n",
    "    def __init__(self, destino=\"dataset_caes\"):\n",
    "        self.destino = Path(destino)\n",
    "        self.racas = [\n",
    "            \"cao_serra_estrela\", \"cao_agua_portugues\", \"podengo_portugues\",\n",
    "            \"perdigueiro_portugues\", \"rafeiro_alentejano\", \"cao_castro_laboreiro\",\n",
    "            \"barbado_terceira\", \"fila_sao_miguel\", \"transmontano\", \"serra_aires\"\n",
    "        ]\n",
    "    \n",
    "    def criar_estrutura(self):\n",
    "        for split in ['train', 'validation']:\n",
    "            for raca in self.racas:\n",
    "                (self.destino / split / raca).mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"\u2713 Estrutura criada em {self.destino}\")\n",
    "    \n",
    "    def verificar(self):\n",
    "        problemas = []\n",
    "        for raca in self.racas:\n",
    "            n_train = len(list((self.destino/'train'/raca).glob('*.jpg')))\n",
    "            n_val = len(list((self.destino/'validation'/raca).glob('*.jpg')))\n",
    "            \n",
    "            print(f\"{raca:25s} - Train: {n_train:3d} | Val: {n_val:3d}\")\n",
    "            \n",
    "            if n_train < 50: problemas.append(f\"{raca}: <50 treino\")\n",
    "            if n_val < 10: problemas.append(f\"{raca}: <10 valida\u00e7\u00e3o\")\n",
    "        \n",
    "        if problemas:\n",
    "            print(\"\\n\u26a0\ufe0f  PROBLEMAS:\")\n",
    "            for p in problemas: print(f\"  - {p}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "# Executar\n",
    "org = OrganizadorDataset()\n",
    "org.criar_estrutura()\n",
    "if not org.verificar():\n",
    "    print(\"\\n\u274c Corrige problemas antes de continuar!\")\n",
    "    exit(1)\n",
    "```\n",
    "\n",
    "**\ud83d\udca1 Explica\u00e7\u00e3o:**\n",
    "- `mkdir(exist_ok=True)` - N\u00e3o d\u00e1 erro se pasta existe\n",
    "- `stratify` - Mant\u00e9m propor\u00e7\u00e3o de classes no split\n",
    "- Verifica\u00e7\u00e3o obrigat\u00f3ria evita treino em dataset inv\u00e1lido\n",
    "\n",
    "### Script 2: Treinar Modelo\n",
    "\n",
    "```python\n",
    "# 2_treinar.py\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# CR\u00cdTICO: Mixed precision economiza 50% RAM\n",
    "keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "class TreinadorSimples:\n",
    "    def __init__(self, dataset_path=\"dataset_caes\"):\n",
    "        self.dataset_path = dataset_path\n",
    "    \n",
    "    def carregar_dados(self, batch_size=32):\n",
    "        \"\"\"\n",
    "        batch_size=32 ideal para M1\n",
    "        Se OOM: reduz para 16\n",
    "        \"\"\"\n",
    "        augmentation = keras.Sequential([\n",
    "            keras.layers.RandomFlip(\"horizontal\"),\n",
    "            keras.layers.RandomRotation(0.2),\n",
    "            keras.layers.RandomZoom(0.2),\n",
    "        ])\n",
    "        \n",
    "        train_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "            f\"{self.dataset_path}/train\",\n",
    "            image_size=(224, 224),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        val_ds = keras.preprocessing.image_dataset_from_directory(\n",
    "            f\"{self.dataset_path}/validation\",\n",
    "            image_size=(224, 224),\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Augmentation + Normaliza\u00e7\u00e3o\n",
    "        train_ds = train_ds.map(lambda x,y: (augmentation(x, training=True)/255, y))\n",
    "        val_ds = val_ds.map(lambda x,y: (x/255, y))\n",
    "        \n",
    "        return train_ds.prefetch(tf.data.AUTOTUNE), val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    def criar_modelo(self, num_classes=10):\n",
    "        \"\"\"EfficientNetB0: leve e eficiente\"\"\"\n",
    "        base = keras.applications.EfficientNetB0(\n",
    "            include_top=False, weights='imagenet', input_shape=(224,224,3)\n",
    "        )\n",
    "        base.trainable = False  # Fase 1: congelado\n",
    "        \n",
    "        model = keras.Sequential([\n",
    "            base,\n",
    "            keras.layers.GlobalAveragePooling2D(),\n",
    "            keras.layers.Dropout(0.3),\n",
    "            keras.layers.Dense(256, activation='relu'),\n",
    "            keras.layers.Dense(num_classes, activation='softmax', dtype='float32')\n",
    "        ])\n",
    "        \n",
    "        return model, base\n",
    "    \n",
    "    def treinar(self, model, base, train_ds, val_ds):\n",
    "        \"\"\"Treino em 2 fases: Transfer Learning \u2192 Fine-tuning\"\"\"\n",
    "        \n",
    "        # FASE 1: S\u00f3 classificador (10-20min)\n",
    "        print(\"\\n\ud83c\udfaf FASE 1: Transfer Learning\")\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(1e-3),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        model.fit(train_ds, validation_data=val_ds, epochs=20, \n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])\n",
    "        \n",
    "        # FASE 2: Fine-tuning \u00faltimas 30 camadas (20-40min)\n",
    "        print(\"\\n\ud83c\udfaf FASE 2: Fine-tuning\")\n",
    "        base.trainable = True\n",
    "        for layer in base.layers[:-30]:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(1e-5),  # LR 100x menor!\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        model.fit(train_ds, validation_data=val_ds, epochs=30,\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)])\n",
    "        \n",
    "        return model\n",
    "\n",
    "# Executar\n",
    "treinador = TreinadorSimples()\n",
    "train_ds, val_ds = treinador.carregar_dados()\n",
    "model, base = treinador.criar_modelo()\n",
    "model = treinador.treinar(model, base, train_ds, val_ds)\n",
    "model.save('classificador_caes.keras')\n",
    "print(\"\u2705 Modelo guardado!\")\n",
    "```\n",
    "\n",
    "**\ud83d\udca1 Pontos-chave:**\n",
    "- **Mixed precision** (FP16): Economiza 50% RAM\n",
    "- **2 fases**: Transfer learning (r\u00e1pido) \u2192 Fine-tuning (preciso)\n",
    "- **Learning rates**: 1e-3 fase 1, 1e-5 fase 2 (importante!)\n",
    "- **Early stopping**: Para automaticamente quando n\u00e3o melhora\n",
    "\n",
    "### Script 3: API REST\n",
    "\n",
    "```python\n",
    "# 3_api.py\n",
    "from flask import Flask, request, jsonify\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = keras.models.load_model('classificador_caes.keras')\n",
    "\n",
    "racas = [\n",
    "    \"C\u00e3o Serra Estrela\", \"C\u00e3o \u00c1gua Portugu\u00eas\", \"Podengo Portugu\u00eas\",\n",
    "    \"Perdigueiro Portugu\u00eas\", \"Rafeiro Alentejano\", \"C\u00e3o Castro Laboreiro\",\n",
    "    \"Barbado Terceira\", \"Fila S\u00e3o Miguel\", \"Transmontano\", \"Serra Aires\"\n",
    "]\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    if 'image' not in request.files:\n",
    "        return jsonify({'erro': 'Sem imagem'}), 400\n",
    "    \n",
    "    img = Image.open(io.BytesIO(request.files['image'].read()))\n",
    "    if img.mode != 'RGB': img = img.convert('RGB')\n",
    "    \n",
    "    img = np.array(img.resize((224,224))) / 255.0\n",
    "    predictions = model.predict(np.expand_dims(img, 0), verbose=0)[0]\n",
    "    \n",
    "    top3 = np.argsort(predictions)[-3:][::-1]\n",
    "    return jsonify({\n",
    "        'previsoes': [\n",
    "            {'raca': racas[i], 'probabilidade': float(predictions[i])}\n",
    "            for i in top3\n",
    "        ]\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(port=5000)\n",
    "```\n",
    "\n",
    "**Testar:** `curl -X POST -F \"image=@teste.jpg\" http://localhost:5000/predict`\n",
    "\n",
    "### \u2705 Checklist\n",
    "- [ ] Dataset \u226550 imgs/classe\n",
    "- [ ] Treino completo sem erros\n",
    "- [ ] Accuracy valida\u00e7\u00e3o >80%\n",
    "- [ ] API responde correctamente\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcdd Projecto 2: An\u00e1lise de Sentimentos\n",
    "\n",
    "### Objectivo\n",
    "Classificar sentimento (Negativo/Neutro/Positivo) com 85%+ accuracy.\n",
    "\n",
    "### Script 1: Dataset\n",
    "\n",
    "```python\n",
    "# 1_criar_dataset.py\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def criar_dataset(n=2000):\n",
    "    exemplos = {\n",
    "        2: [\"Excelente!\", \"Adorei!\", \"Recomendo!\"],  # Positivo\n",
    "        1: [\"Razo\u00e1vel.\", \"Est\u00e1 bem.\", \"Normal.\"],    # Neutro\n",
    "        0: [\"P\u00e9ssimo!\", \"Horr\u00edvel!\", \"N\u00e3o comprem!\"] # Negativo\n",
    "    }\n",
    "    \n",
    "    dados = []\n",
    "    for label, textos in exemplos.items():\n",
    "        for _ in range(n//3):\n",
    "            dados.append({'texto': random.choice(textos), 'label': label})\n",
    "    \n",
    "    df = pd.DataFrame(dados).sample(frac=1, random_state=42)\n",
    "    df.to_csv('reviews.csv', index=False)\n",
    "    print(f\"\u2713 Dataset: {len(df)} exemplos\")\n",
    "    print(df['label'].value_counts())\n",
    "\n",
    "criar_dataset()\n",
    "```\n",
    "\n",
    "### Script 2: Treinar\n",
    "\n",
    "```python\n",
    "# 2_treinar_bert.py\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Mixed precision\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Dados\n",
    "df = pd.read_csv('reviews.csv')\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "# BERT Portugu\u00eas (400MB download primeira vez)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"neuralmind/bert-base-portuguese-cased\", num_labels=3\n",
    ")\n",
    "\n",
    "# Tokenizar\n",
    "def prep_dataset(df, batch_size=16):\n",
    "    encodings = tokenizer(df['texto'].tolist(), truncation=True, padding=True, \n",
    "                          max_length=128, return_tensors='tf')\n",
    "    return tf.data.Dataset.from_tensor_slices((\n",
    "        dict(encodings), df['label'].values\n",
    "    )).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = prep_dataset(train_df)\n",
    "val_ds = prep_dataset(val_df)\n",
    "\n",
    "# Treinar (15-20min, 3 epochs suficientes)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=3)\n",
    "model.save_pretrained('modelo_sentimentos')\n",
    "tokenizer.save_pretrained('modelo_sentimentos')\n",
    "print(\"\u2705 Modelo guardado!\")\n",
    "```\n",
    "\n",
    "**\ud83d\udca1 Importante:**\n",
    "- `max_length=128` (n\u00e3o 512) economiza RAM\n",
    "- `batch_size=16` para M1 16GB\n",
    "- `2e-5` learning rate padr\u00e3o para BERT\n",
    "\n",
    "### Script 3: Interface Streamlit\n",
    "\n",
    "```python\n",
    "# 3_app.py\n",
    "import streamlit as st\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "st.set_page_config(page_title=\"Sentimentos PT\", page_icon=\"\ud83d\ude0a\")\n",
    "\n",
    "@st.cache_resource\n",
    "def carregar():\n",
    "    tokenizer = AutoTokenizer.from_pretrained('modelo_sentimentos')\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained('modelo_sentimentos')\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = carregar()\n",
    "\n",
    "st.title(\"\ud83d\ude0a An\u00e1lise de Sentimentos\")\n",
    "texto = st.text_area(\"Texto:\", height=150)\n",
    "\n",
    "if st.button(\"Analisar\") and texto:\n",
    "    inputs = tokenizer(texto, return_tensors='tf', truncation=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    probs = tf.nn.softmax(outputs.logits, axis=-1).numpy()[0]\n",
    "    \n",
    "    labels = ['Negativo \ud83d\ude22', 'Neutro \ud83d\ude10', 'Positivo \ud83d\ude0a']\n",
    "    resultado = labels[probs.argmax()]\n",
    "    \n",
    "    st.markdown(f\"## {resultado}\")\n",
    "    st.markdown(f\"**Confian\u00e7a:** {probs.max()*100:.1f}%\")\n",
    "    \n",
    "    import plotly.graph_objects as go\n",
    "    fig = go.Figure(data=[go.Bar(x=labels, y=probs*100)])\n",
    "    st.plotly_chart(fig)\n",
    "```\n",
    "\n",
    "**Executar:** `streamlit run 3_app.py`\n",
    "\n",
    "### \u2705 Checklist\n",
    "- [ ] Dataset balanceado (\u2265300)\n",
    "- [ ] Treino 3 epochs completo\n",
    "- [ ] Accuracy >80%\n",
    "- [ ] Interface funcional\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83e\udd16 Projecto 3: Fine-tuning LLM\n",
    "\n",
    "### Objectivo\n",
    "Fine-tuning Mistral 7B com LoRA para dom\u00ednio espec\u00edfico (Python).\n",
    "\n",
    "### Script 1: Dataset\n",
    "\n",
    "```python\n",
    "# 1_dataset.py\n",
    "import json\n",
    "\n",
    "exemplos = [\n",
    "    {\"instruction\": \"Como criar lista?\", \n",
    "     \"output\": \"Use colchetes:\\n```python\\nlista = [1, 2, 3]\\n```\"},\n",
    "    {\"instruction\": \"O que \u00e9 dicion\u00e1rio?\",\n",
    "     \"output\": \"Pares chave-valor:\\n```python\\nd = {'nome': 'Jo\u00e3o'}\\n```\"},\n",
    "    # Adiciona 100+ exemplos!\n",
    "]\n",
    "\n",
    "# Guardar JSONL\n",
    "with open('train.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for ex in exemplos[:int(len(exemplos)*0.9)]:\n",
    "        texto = f\"### Instru\u00e7\u00e3o:\\n{ex['instruction']}\\n\\n### Resposta:\\n{ex['output']}\"\n",
    "        f.write(json.dumps({\"text\": texto}, ensure_ascii=False) + '\\n')\n",
    "\n",
    "with open('val.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for ex in exemplos[int(len(exemplos)*0.9):]:\n",
    "        texto = f\"### Instru\u00e7\u00e3o:\\n{ex['instruction']}\\n\\n### Resposta:\\n{ex['output']}\"\n",
    "        f.write(json.dumps({\"text\": texto}, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"\u2713 {len(exemplos)} exemplos criados\")\n",
    "```\n",
    "\n",
    "### Script 2: Fine-tuning MLX\n",
    "\n",
    "```python\n",
    "# 2_treinar_mlx.py\n",
    "from mlx_lm.tuner import train\n",
    "\n",
    "# Configura\u00e7\u00e3o optimizada M1 16GB\n",
    "config = {\n",
    "    \"lora_layers\": 16,      # Camadas a adaptar\n",
    "    \"lora_rank\": 16,        # Tamanho adaptadores\n",
    "    \"lora_alpha\": 32,       # Scaling\n",
    "    \"batch_size\": 2,        # CR\u00cdTICO: n\u00e3o aumentar!\n",
    "    \"iters\": 1000,          # ~3 epochs\n",
    "    \"learning_rate\": 1e-5,  # Padr\u00e3o LoRA\n",
    "    \"adapter_file\": \"adapters.npz\"\n",
    "}\n",
    "\n",
    "print(\"\ud83d\ude80 Fine-tuning (30-60min)...\")\n",
    "train(\n",
    "    model=\"mlx-community/Mistral-7B-v0.1-4bit\",\n",
    "    data=\"train.jsonl\",\n",
    "    valid_data=\"val.jsonl\",\n",
    "    **config\n",
    ")\n",
    "print(\"\u2705 Adaptadores guardados!\")\n",
    "```\n",
    "\n",
    "**\ud83d\udca1 Mem\u00f3ria:**\n",
    "- Base 4-bit: ~3.5GB\n",
    "- LoRA params: ~20MB\n",
    "- Activa\u00e7\u00f5es: ~2GB\n",
    "- **Total: ~6GB** \u2705 Cabe no M1!\n",
    "\n",
    "### Script 3: Chat Interface\n",
    "\n",
    "```python\n",
    "# 3_chat.py\n",
    "import streamlit as st\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "st.title(\"\ud83d\udc0d Assistente Python\")\n",
    "\n",
    "@st.cache_resource\n",
    "def carregar():\n",
    "    return load(\"mlx-community/Mistral-7B-v0.1-4bit\", adapter_file=\"adapters.npz\")\n",
    "\n",
    "model, tokenizer = carregar()\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for msg in st.session_state.messages:\n",
    "    with st.chat_message(msg[\"role\"]):\n",
    "        st.markdown(msg[\"content\"])\n",
    "\n",
    "if prompt := st.chat_input(\"Pergunta...\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    \n",
    "    with st.chat_message(\"assistant\"):\n",
    "        prompt_fmt = f\"### Instru\u00e7\u00e3o:\\n{prompt}\\n\\n### Resposta:\\n\"\n",
    "        resposta = generate(model, tokenizer, prompt=prompt_fmt, max_tokens=200, temp=0.7)\n",
    "        resposta = resposta.split(\"### Resposta:\\n\")[-1].strip()\n",
    "        st.markdown(resposta)\n",
    "    \n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": resposta})\n",
    "```\n",
    "\n",
    "### \u2705 Checklist\n",
    "- [ ] Dataset \u2265100 exemplos qualidade\n",
    "- [ ] Fine-tuning completo\n",
    "- [ ] Testes mostram melhoria\n",
    "- [ ] Chat interface funcional\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udea8 Troubleshooting R\u00e1pido\n",
    "\n",
    "### Out of Memory\n",
    "```python\n",
    "# Reduz batch_size\n",
    "batch_size = 16  # era 32\n",
    "# Ou fecha apps e reinicia\n",
    "```\n",
    "\n",
    "### GPU n\u00e3o detectada\n",
    "```bash\n",
    "pip uninstall tensorflow-metal\n",
    "pip install tensorflow-metal==1.1.0\n",
    "```\n",
    "\n",
    "### Modelo n\u00e3o guarda\n",
    "```python\n",
    "import os\n",
    "os.makedirs('modelos', exist_ok=True)\n",
    "model.save('modelos/modelo.keras')\n",
    "```\n",
    "\n",
    "### Treino muito lento\n",
    "```python\n",
    "# Verifica GPU\n",
    "import tensorflow as tf\n",
    "print(len(tf.config.list_physical_devices('GPU')))\n",
    "# Deve ser >0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcca Resumo Final\n",
    "\n",
    "**Completaste os 3 projectos!** \ud83c\udf89\n",
    "\n",
    "Agora dominas:\n",
    "- \u2705 Transfer learning e fine-tuning\n",
    "- \u2705 NLP com transformers\n",
    "- \u2705 LLMs com LoRA/QLoRA\n",
    "- \u2705 Optimiza\u00e7\u00e3o para M1 16GB\n",
    "- \u2705 Deployment (API + Streamlit)\n",
    "\n",
    "**Pr\u00f3ximos passos:**\n",
    "1. Adapta ao teu dom\u00ednio\n",
    "2. Experimenta outros modelos\n",
    "3. Combina t\u00e9cnicas\n",
    "4. Deploy em produ\u00e7\u00e3o\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}