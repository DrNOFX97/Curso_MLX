{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M√≥dulo 5: Modelos de Linguagem Grandes (LLMs)\n",
    "\n",
    "### 5.1 Limita√ß√µes e Estrat√©gias\n",
    "\n",
    "#### Por que 16GB √© Limitante para LLMs?\n",
    "\n",
    "**Requisitos t√≠picos de mem√≥ria para LLMs:**\n",
    "\n",
    "| Modelo | Par√¢metros | FP32 | FP16 | INT8 | INT4 |\n",
    "|--------|-----------|------|------|------|------|\n",
    "| GPT-2 Small | 124M | 500MB | 250MB | 125MB | 63MB ‚úÖ |\n",
    "| GPT-2 Medium | 355M | 1.4GB | 700MB | 350MB | 175MB ‚úÖ |\n",
    "| LLaMA-2 7B | 7B | 28GB ‚ùå | 14GB ‚ö†Ô∏è | 7GB ‚úÖ | 3.5GB ‚úÖ |\n",
    "| LLaMA-2 13B | 13B | 52GB ‚ùå | 26GB ‚ùå | 13GB ‚ö†Ô∏è | 6.5GB ‚úÖ |\n",
    "| Mistral 7B | 7B | 28GB ‚ùå | 14GB ‚ö†Ô∏è | 7GB ‚úÖ | 3.5GB ‚úÖ |\n",
    "\n",
    "**Realidade no M1 16GB:**\n",
    "- Sistema operativo: ~3-4GB\n",
    "- Apps em background: ~1-2GB\n",
    "- **Dispon√≠vel para modelo: ~10-12GB**\n",
    "\n",
    "**Conclus√£o:**\n",
    "- ‚úÖ Modelos at√© 7B: Vi√°veis com quantiza√ß√£o INT4/INT8\n",
    "- ‚ö†Ô∏è Modelos 13B: Poss√≠vel apenas com INT4 + otimiza√ß√µes\n",
    "- ‚ùå Modelos 30B+: Imposs√≠vel carregar completos na mem√≥ria\n",
    "\n",
    "#### Estrat√©gias para Trabalhar com LLMs no M1\n",
    "\n",
    "**1. Quantiza√ß√£o Extrema (4-bit, GGUF)**\n",
    "\n",
    "GGUF = GPT-Generated Unified Format (sucessor de GGML)\n",
    "- Formato otimizado para infer√™ncia em CPU/GPU unificada\n",
    "- Quantiza√ß√£o 4-bit com grupos (mant√©m qualidade)\n",
    "- Ideal para Apple Silicon\n",
    "\n",
    "**2. LoRA (Low-Rank Adaptation)**\n",
    "\n",
    "Em vez de treinar todos os par√¢metros, adiciona matrizes pequenas:\n",
    "```\n",
    "Modelo original:  7B par√¢metros (frozen)\n",
    "     +\n",
    "LoRA adapters:    ~10M par√¢metros (trein√°veis)\n",
    "     =\n",
    "Modelo fine-tuned mantendo 99.9% frozen!\n",
    "```\n",
    "\n",
    "**Economia de mem√≥ria:**\n",
    "- Treino normal de 7B: ~40GB necess√°rios (gradientes + optimizador)\n",
    "- Treino com LoRA: ~12GB necess√°rios ‚úÖ\n",
    "\n",
    "**3. QLoRA (Quantized LoRA)**\n",
    "\n",
    "Combina quantiza√ß√£o + LoRA:\n",
    "```\n",
    "Base model em INT4:  3.5GB\n",
    "LoRA params em FP16: 20MB\n",
    "Gradientes:          ~2GB\n",
    "Total:               ~6GB ‚úÖ Cabe no M1!\n",
    "```\n",
    "\n",
    "#### Formato GGUF - Explica√ß√£o T√©cnica\n",
    "\n",
    "```python\n",
    "# compreender_gguf.py\n",
    "\"\"\"\n",
    "GGUF (GPT-Generated Unified Format)\n",
    "\n",
    "Caracter√≠sticas:\n",
    "- Formato bin√°rio otimizado\n",
    "- Suporta quantiza√ß√£o 2-bit at√© 8-bit\n",
    "- Metadados inclu√≠dos (arquitetura, vocabul√°rio)\n",
    "- Carregamento r√°pido (mmap)\n",
    "- Ideal para llama.cpp e MLX\n",
    "\"\"\"\n",
    "\n",
    "# Tipos de quantiza√ß√£o GGUF:\n",
    "quantizacoes = {\n",
    "    'Q2_K': '2-bit, muito compacto, perda qualidade',\n",
    "    'Q3_K_S': '3-bit small, bom compromisso',\n",
    "    'Q4_0': '4-bit, recomendado geral',\n",
    "    'Q4_K_M': '4-bit medium, melhor qualidade',\n",
    "    'Q5_0': '5-bit, alta qualidade',\n",
    "    'Q5_K_M': '5-bit medium, muito boa',\n",
    "    'Q8_0': '8-bit, quase sem perda',\n",
    "}\n",
    "\n",
    "# Escolha para M1 16GB:\n",
    "# - Q4_K_M: Melhor equil√≠brio qualidade/tamanho\n",
    "# - Q5_K_M: Se tiveres espa√ßo extra\n",
    "# - Q8_0: Apenas modelos pequenos (<3B)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Fine-tuning de LLMs Pequenos\n",
    "\n",
    "#### Cuidados com Datasets e Formatos Ideais\n",
    "\n",
    "**‚ö†Ô∏è CR√çTICO: A qualidade do dataset √© mais importante que a quantidade!**\n",
    "\n",
    "#### Princ√≠pios Fundamentais\n",
    "\n",
    "**1. Qualidade sobre Quantidade**\n",
    "\n",
    "```python\n",
    "# Exemplo de dataset de baixa qualidade:\n",
    "# Respostas gen√©ricas, erros, etc.\n",
    "\n",
    "# Exemplo de dataset de alta qualidade:\n",
    "# Respostas precisas, bem escritas, diversas.\n",
    "```\n",
    "\n",
    "**2. Diversidade √© Essencial**\n",
    "\n",
    "```python\n",
    "# analisar_diversidade.py\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def analisar_diversidade(dataset):\n",
    "    \"\"\"\n",
    "    Analisa diversos aspetos do dataset\n",
    "    \"\"\"\n",
    "    # ... (c√≥digo de an√°lise de comprimento, palavras iniciais, t√≥picos, duplicados)\n",
    "    pass\n",
    "\n",
    "# Se uma palavra aparece >30%, h√° falta de diversidade!\n",
    "# Se categorias est√£o desbalanceadas, usar balancear_categorias()\n",
    "```\n",
    "\n",
    "**3. Balanceamento**\n",
    "\n",
    "```python\n",
    "# balancear_dataset.py\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "def balancear_categorias(dataset, max_por_categoria=100):\n",
    "    \"\"\"\n",
    "    Balanceia dataset para evitar overfitting em categorias sobre-representadas\n",
    "    \"\"\"\n",
    "    # ... (c√≥digo para agrupar por categoria e amostrar)\n",
    "    pass\n",
    "\n",
    "# Uso\n",
    "dataset_balanceado = balancear_categorias(dataset, max_por_categoria=200)\n",
    "```\n",
    "\n",
    "#### Formatos de Dataset Ideais\n",
    "\n",
    "**Formato 1: Alpaca (Mais Comum)**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"Explica o que √© aprendizagem autom√°tica\",\n",
    "  \"input\": \"\",\n",
    "  \"output\": \"Aprendizagem autom√°tica √© um ramo da intelig√™ncia artificial que permite aos computadores aprender a partir de dados sem serem explicitamente programados...\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Quando usar:**\n",
    "- Fine-tuning de instru√ß√µes\n",
    "- Q&A geral\n",
    "- Tarefas de seguir comandos\n",
    "\n",
    "**Formato em texto:**\n",
    "\n",
    "```python\n",
    "# formato_alpaca.py\n",
    "def formatar_prompt(instrucao, resposta, contexto=\"\"):\n",
    "    \"\"\"Formato Alpaca padr√£o\"\"\"\n",
    "    if contexto:\n",
    "        prompt = f\"### Instru√ß√£o:\\n{instrucao}\\n\\n### Contexto:\\n{contexto}\\n\\n### Resposta:\\n{resposta}\" \n",
    "    else:\n",
    "        prompt = f\"### Instru√ß√£o:\\n{instrucao}\\n\\n### Resposta:\\n{resposta}\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Exemplo\n",
    "exemplo = formatar_prompt(\n",
    "    instrucao=\"Traduz para ingl√™s: Ol√°, como est√°s?\",\n",
    "    resposta=\"Hello, how are you?\"\n",
    ")\n",
    "print(exemplo)\n",
    "```\n",
    "\n",
    "**Formato 2: ChatML (Para Conversas)**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"√âs um assistente prest√°vel em portugu√™s.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Como fazer um bolo?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Para fazer um bolo: 1. ...\"}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Quando usar:**\n",
    "- Conversas multi-turn\n",
    "- Assistentes com personalidade\n",
    "- Contexto de sistema importante\n",
    "\n",
    "**Implementa√ß√£o:**\n",
    "\n",
    "```python\n",
    "# formato_chatml.py\n",
    "def formatar_chatml(mensagens):\n",
    "    \"\"\"\n",
    "    Formato ChatML (usado por GPT, Mistral, etc.)\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    \n",
    "    for msg in mensagens:\n",
    "        role = msg['role']\n",
    "        content = msg['content']\n",
    "        \n",
    "        if role == \"system\":\n",
    "            prompt += f\"<|im_start|>system\\n{content}<|im_end|>\\n\"\n",
    "        elif role == \"user\":\n",
    "            prompt += f\"<|im_start|>user\\n{content}<|im_end|>\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            prompt += f\"<|im_start|>assistant\\n{content}<|im_end|>\\n\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Exemplo de conversa\n",
    "conversa = [\n",
    "    {\"role\": \"system\", \"content\": \"√âs um tutor de matem√°tica.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Quanto √© 2+2?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"2+2 = 4\"},\n",
    "    {\"role\": \"user\", \"content\": \"E 5*3?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"5*3 = 15\"}\n",
    "]\n",
    "\n",
    "prompt = formatar_chatml(conversa)\n",
    "```\n",
    "\n",
    "**Formato 3: Completion (Mais Simples)**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"text\": \"Pergunta: O que √© Python?\\n\\nResposta: Python √© uma linguagem...\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Quando usar:**\n",
    "- Datasets muito simples\n",
    "- Continuar texto\n",
    "- Estilo liter√°rio\n",
    "\n",
    "#### Tamanho Ideal do Dataset\n",
    "\n",
    "**Regras gerais:**\n",
    "\n",
    "| Tipo de Fine-tuning | M√≠nimo | Recomendado | Ideal |\n",
    "|---------------------|--------|-------------|-------|\n",
    "| **Adapta√ß√£o de estilo** | 50 | 200 | 500 |\n",
    "| **Dom√≠nio espec√≠fico** | 200 | 1.000 | 5.000 |\n",
    "| **Novo conhecimento** | 1.000 | 5.000 | 10.000+ |\n",
    "| **Mudan√ßa de comportamento** | 500 | 2.000 | 5.000 |\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANTE: Com LoRA no M1, datasets grandes (>5k) podem ser problem√°ticos**\n",
    "\n",
    "```python\n",
    "# calcular_tempo_treino.py\n",
    "def estimar_tempo_treino(num_exemplos, epochs=3, batch_size=4):\n",
    "    \"\"\"\n",
    "    Estima tempo de treino no M1 16GB\n",
    "    \"\"\"\n",
    "    steps_por_epoch = num_exemplos // batch_size\n",
    "    total_steps = steps_por_epoch * epochs\n",
    "    \n",
    "    # M1 processa ~2-3 steps/segundo para LLaMA 7B com LoRA\n",
    "    tempo_segundos = total_steps / 2.5\n",
    "    \n",
    "    horas = tempo_segundos / 3600\n",
    "    \n",
    "    print(f\"üìä Estimativa de Treino\")\n",
    "    print(f\"  Exemplos: {num_exemplos}\")\n",
    "    print(f\"  Epochs: {epochs}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Total steps: {total_steps}\")\n",
    "    print(f\"  Tempo estimado: {horas:.1f} horas\")\n",
    "    \n",
    "    if horas > 12:\n",
    "        print(f\"\\n‚ö†Ô∏è  Considera reduzir dataset ou epochs!\")\n",
    "\n",
    "# Testa\n",
    "estimar_tempo_treino(num_exemplos=5000, epochs=3, batch_size=4)\n",
    "```\n",
    "\n",
    "#### Cuidados Cr√≠ticos\n",
    "\n",
    "**1. Evitar Data Leakage**\n",
    "\n",
    "```python\n",
    "# split_correto.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset_correto(dataset):\n",
    "    \"\"\"\n",
    "    Split correto para evitar leakage\n",
    "    \"\"\"\n",
    "    # ‚ùå ERRADO: Split aleat√≥rio se houver conversas multi-turn\n",
    "    # Uma parte da conversa no treino, outra no teste = leakage!\n",
    "    \n",
    "    # ‚úÖ CORRETO: Split por \"conversa_id\" ou \"user_id\"\n",
    "    \n",
    "    # Agrupar por conversa\n",
    "    conversas = {}\n",
    "    for exemplo in dataset:\n",
    "        conv_id = exemplo.get('conversa_id', exemplo.get('id'))\n",
    "        if conv_id not in conversas:\n",
    "            conversas[conv_id] = []\n",
    "        conversas[conv_id].append(exemplo)\n",
    "    \n",
    "    # Split por conversas completas\n",
    "    conv_ids = list(conversas.keys())\n",
    "    train_ids, test_ids = train_test_split(conv_ids, test_size=0.2, random_state=42)\n",
    "    \n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for conv_id in train_ids:\n",
    "        train_data.extend(conversas[conv_id])\n",
    "    \n",
    "    for conv_id in test_ids:\n",
    "        test_data.extend(conversas[conv_id])\n",
    "    \n",
    "    print(f\"‚úì Split correto:\")\n",
    "    print(f\"  Treino: {len(train_data)} exemplos ({len(train_ids)} conversas)\")\n",
    "    print(f\"  Teste:  {len(test_data)} exemplos ({len(test_ids)} conversas)\")\n",
    "    \n",
    "    return train_data, test_data\n",
    "```\n",
    "\n",
    "**2. Limpeza de Dados**\n",
    "\n",
    "```python\n",
    "# limpar_dataset.py\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def limpar_texto(texto):\n",
    "    \"\"\"\n",
    "    Limpa texto removendo problemas comuns\n",
    "    \"\"\"\n",
    "    # Normalizar unicode\n",
    "    texto = unicodedata.normalize('NFKC', texto)\n",
    "    \n",
    "    # Remover espa√ßos m√∫ltiplos\n",
    "    texto = re.sub(r'\\s+', ' ', texto)\n",
    "    \n",
    "    # Remover caracteres de controlo (excepto \\n)\n",
    "    texto = ''.join(char for char in texto if char == '\\n' or not unicodedata.category(char).startswith('C'))\n",
    "    \n",
    "    # Remover linhas vazias m√∫ltiplas\n",
    "    texto = re.sub(r'\\n\\s*\\n', '\\n\\n', texto)\n",
    "    \n",
    "    # Trim\n",
    "    texto = texto.strip()\n",
    "    \n",
    "    return texto\n",
    "\n",
    "def validar_exemplo(exemplo, min_length=10, max_length=2048):\n",
    "    \"\"\"\n",
    "    Valida se exemplo √© adequado\n",
    "    \"\"\"\n",
    "    instrucao = exemplo.get('instrucao', '')\n",
    "    resposta = exemplo.get('resposta', '')\n",
    "    \n",
    "    # Verifica√ß√µes\n",
    "    if len(resposta) < min_length:\n",
    "        return False, \"Resposta muito curta\"\n",
    "    \n",
    "    if len(resposta) > max_length:\n",
    "        return False, \"Resposta muito longa\"\n",
    "    \n",
    "    if not instrucao or not resposta:\n",
    "        return False, \"Campos vazios\"\n",
    "    \n",
    "    # Verificar se n√£o √© s√≥ pontua√ß√£o\n",
    "    if len(re.findall(r'\\w+', resposta)) < 3:\n",
    "        return False, \"Resposta sem conte√∫do\"\n",
    "    \n",
    "    # Verificar URLs suspeitas (poss√≠vel spam)\n",
    "    if len(re.findall(r'http[s]?:\\/\\/(?:[a-zA-Z]|[0-9]|[$-_@.&+])+', resposta)) > 2:\n",
    "        return False, \"Muitas URLs\"\n",
    "    \n",
    "    return True, \"OK\"\n",
    "\n",
    "def limpar_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Limpa e valida dataset completo\n",
    "    \"\"\"\n",
    "    dataset_limpo = []\n",
    "    estatisticas = {'removidos': 0, 'mantidos': 0, 'motivos': {}}\n",
    "    \n",
    "    for exemplo in dataset:\n",
    "        # Limpar textos\n",
    "        if 'instrucao' in exemplo:\n",
    "            exemplo['instrucao'] = limpar_texto(exemplo['instrucao'])\n",
    "        if 'resposta' in exemplo:\n",
    "            exemplo['resposta'] = limpar_texto(exemplo['resposta'])\n",
    "        \n",
    "        # Validar\n",
    "        valido, motivo = validar_exemplo(exemplo)\n",
    "        \n",
    "        if valido:\n",
    "            dataset_limpo.append(exemplo)\n",
    "            estatisticas['mantidos'] += 1\n",
    "        else:\n",
    "            estatisticas['removidos'] += 1\n",
    "            estatisticas['motivos'][motivo] = estatisticas['motivos'].get(motivo, 0) + 1\n",
    "    \n",
    "    # Relat√≥rio\n",
    "    print(\"üßπ LIMPEZA DE DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total original: {len(dataset)}\")\n",
    "    print(f\"Mantidos:       {estatisticas['mantidos']}\")\n",
    "    print(f\"Removidos:      {estatisticas['removidos']}\")\n",
    "    \n",
    "    if estatisticas['motivos']:\n",
    "        print(f\"\\nMotivos de remo√ß√£o:\")\n",
    "        for motivo, count in estatisticas['motivos'].items():\n",
    "            print(f\"  {motivo}: {count}\")\n",
    "    \n",
    "    return dataset_limpo\n",
    "\n",
    "# Uso\n",
    "dataset_limpo = limpar_dataset(dataset_original)\n",
    "```\n",
    "\n",
    "**3. Tokeniza√ß√£o e Comprimento**\n",
    "\n",
    "```python\n",
    "# analisar_tokens.py\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def analisar_comprimento_tokens(dataset, model_name=\"meta-llama/Llama-2-7b-hf\"):\n",
    "    \"\"\"\n",
    "    Analisa comprimento em tokens (n√£o palavras!)\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    comprimentos = []\n",
    "    muito_longos = 0\n",
    "    max_length = 2048  # Limite t√≠pico\n",
    "    \n",
    "    for exemplo in dataset:\n",
    "        texto_completo = formatar_prompt(\n",
    "            exemplo['instrucao'],\n",
    "            exemplo['resposta']\n",
    "        )\n",
    "        \n",
    "        tokens = tokenizer.encode(texto_completo)\n",
    "        comprimentos.append(len(tokens))\n",
    "        \n",
    "        if len(tokens) > max_length:\n",
    "            muito_longos += 1\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"üìè AN√ÅLISE DE COMPRIMENTO (TOKENS)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"M√≠nimo:    {min(comprimentos)} tokens\")\n",
    "    print(f\"M√°ximo:    {max(comprimentos)} tokens\")\n",
    "    print(f\"M√©dia:     {np.mean(comprimentos):.0f} tokens\")\n",
    "    print(f\"Mediana:   {np.median(comprimentos):.0f} tokens\")\n",
    "    print(f\"P95:       {np.percentile(comprimentos, 95):.0f} tokens\")\n",
    "    \n",
    "    if muito_longos > 0:\n",
    "        pct = 100 * muito_longos / len(dataset)\n",
    "        print(f\"\\n‚ö†Ô∏è  {muito_longos} exemplos ({pct:.1f}%) excedem {max_length} tokens\")\n",
    "        print(f\"    Considera truncar ou remover estes exemplos\")\n",
    "    \n",
    "    # Recomenda√ß√£o de max_length\n",
    "    p95 = int(np.percentile(comprimentos, 95))\n",
    "    max_recomendado = min(2048, ((p95 // 128) + 1) * 128)  # Arredondar a m√∫ltiplo de 128\n",
    "    \n",
    "    print(f\"\\n‚úÖ max_length recomendado: {max_recomendado}\")\n",
    "    \n",
    "    return comprimentos, max_recomendado\n",
    "\n",
    "# Uso\n",
    "comprimentos, max_len = analisar_comprimento_tokens(dataset)\n",
    "```\n",
    "\n",
    "#### Template de Prepara√ß√£o Completa\n",
    "\n",
    "```python\n",
    "# preparar_dataset_completo.py\n",
    "\"\"\"\n",
    "Pipeline completo de prepara√ß√£o de dataset\n",
    "\"\"\"\n",
    "\n",
    "class PreparadorDataset:\n",
    "    def __init__(self, formato='alpaca'):\n",
    "        self.formato = formato\n",
    "        self.estatisticas = {}\n",
    "    \n",
    "    def pipeline_completo(self, dados_brutos, output_file=\"dataset_preparado.jsonl\"):\n",
    "        \"\"\"\n",
    "        Pipeline completo: validar ‚Üí limpar ‚Üí balancear ‚Üí analisar ‚Üí guardar\n",
    "        \"\"\"\n",
    "        print(\"üöÄ INICIANDO PREPARA√á√ÉO DE DATASET\\n\")\n",
    "        \n",
    "        # 1. Limpeza\n",
    "        print(\"PASSO 1: Limpeza\")\n",
    "        dados_limpos = limpar_dataset(dados_brutos)\n",
    "        print()\n",
    "        \n",
    "        # 2. Balanceamento\n",
    "        print(\"PASSO 2: Balanceamento\")\n",
    "        dados_balanceados = balancear_categorias(dados_limpos)\n",
    "        print()\n",
    "        \n",
    "        # 3. An√°lise de diversidade\n",
    "        print(\"PASSO 3: An√°lise de Diversidade\")\n",
    "        metricas = analisar_diversidade(dados_balanceados)\n",
    "        print()\n",
    "        \n",
    "        # 4. An√°lise de tokens\n",
    "        print(\"PASSO 4: An√°lise de Tokens\")\n",
    "        comprimentos, max_len = analisar_comprimento_tokens(dados_balanceados)\n",
    "        print()\n",
    "        \n",
    "        # 5. Split treino/valida√ß√£o\n",
    "        print(\"PASSO 5: Split Treino/Valida√ß√£o\")\n",
    "        treino, validacao = split_dataset_correto(dados_balanceados)\n",
    "        print()\n",
    "        \n",
    "        # 6. Guardar\n",
    "        print(\"PASSO 6: Guardar Dataset\")\n",
    "        import json\n",
    "        \n",
    "        with open(f\"train_{output_file}\", 'w', encoding='utf-8') as f:\n",
    "            for exemplo in treino:\n",
    "                f.write(json.dumps(exemplo, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        with open(f\"val_{output_file}\", 'w', encoding='utf-8') as f:\n",
    "            for exemplo in validacao:\n",
    "                f.write(json.dumps(exemplo, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"‚úì Guardado em train_{output_file} e val_{output_file}\")\n",
    "        \n",
    "        # Relat√≥rio final\n",
    "        self.relatorio_final(treino, validacao, max_len)\n",
    "        \n",
    "        return treino, validacao\n",
    "    \n",
    "    def relatorio_final(self, treino, validacao, max_len):\n",
    "        \"\"\"\n",
    "        Relat√≥rio final com recomenda√ß√µes\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üìã RELAT√ìRIO FINAL\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Dataset de treino:   {len(treino)} exemplos\")\n",
    "        print(f\"Dataset de valida√ß√£o: {len(validacao)} exemplos\")\n",
    "        print(f\"max_length sugerido: {max_len}\")\n",
    "        \n",
    "        # Estimativa de tempo\n",
    "        print(\"\\n‚è±Ô∏è  Estimativa de Treino (3 epochs, batch_size=4):\")\n",
    "        estimar_tempo_treino(len(treino), epochs=3, batch_size=4)\n",
    "        \n",
    "        print(\"\\n‚úÖ Dataset pronto para treino!\")\n",
    "        print(\"   Pr√≥ximo passo: Fine-tuning com LoRA\")\n",
    "\n",
    "# Uso\n",
    "preparador = PreparadorDataset(formato='alpaca')\n",
    "train_data, val_data = preparador.pipeline_completo(dados_brutos)\n",
    "```\n",
    "\n",
    "#### Checklist de Qualidade do Dataset\n",
    "\n",
    "```\n",
    "‚úÖ QUALIDADE\n",
    "   [ ] Respostas revistas manualmente (amostra de 10%)\n",
    "   [ ] Sem erros ortogr√°ficos ou gramaticais\n",
    "   [ ] Informa√ß√£o factualmente correta\n",
    "   [ ] Tom e estilo consistentes\n",
    "\n",
    "‚úÖ DIVERSIDADE\n",
    "   [ ] T√≥picos variados\n",
    "   [ ] Comprimentos diversos (curto, m√©dio, longo)\n",
    "   [ ] Diferentes tipos de perguntas\n",
    "   [ ] Vocabul√°rio rico\n",
    "\n",
    "‚úÖ FORMATO\n",
    "   [ ] Formato consistente (Alpaca/ChatML)\n",
    "   [ ] Campos obrigat√≥rios preenchidos\n",
    "   [ ] Encoding UTF-8\n",
    "   [ ] Linha por exemplo (JSONL)\n",
    "\n",
    "‚úÖ TAMANHO\n",
    "   [ ] M√≠nimo de exemplos atingido (50-200 dependendo do caso)\n",
    "   [ ] N√£o excessivamente grande (>10k pode ser contraproducente)\n",
    "   [ ] Balanceado entre categorias\n",
    "\n",
    "‚úÖ T√âCNICO\n",
    "   [ ] Sem duplicados\n",
    "   [ ] Split treino/valida√ß√£o correto (sem leakage)\n",
    "   [ ] Comprimento de tokens analisado\n",
    "   [ ] max_length definido apropriadamente\n",
    "\n",
    "‚úÖ √âTICA\n",
    "   [ ] Sem conte√∫do ofensivo ou discriminat√≥rio\n",
    "   [ ] Sem informa√ß√£o pessoal identific√°vel\n",
    "   [ ] Sem dados com copyright\n",
    "   [ ] Consentimento para uso dos dados (se aplic√°vel)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Modelos at√© 7B par√¢metros\n",
    "\n",
    "```bash\n",
    "# Instalar depend√™ncias\n",
    "pip install transformers==4.36.0\n",
    "pip install peft==0.7.1              # Para LoRA\n",
    "pip install bitsandbytes==0.41.3     # Para quantiza√ß√£o\n",
    "pip install datasets\n",
    "pip install accelerate\n",
    "pip install trl                      # Para RLHF/SFT\n",
    "```\n",
    "\n",
    "#### Fine-tuning com LoRA - Exemplo Completo\n",
    "\n",
    "**Dataset: Instru√ß√µes em Portugu√™s**\n",
    "\n",
    "```python\n",
    "# preparar_dataset_pt.py\n",
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# Exemplo: Dataset de perguntas e respostas\n",
    "dados_treino = [\n",
    "    {\n",
    "        \"instrucao\": \"Explica o que √© aprendizagem autom√°tica\",\n",
    "        \"resposta\": \"Aprendizagem autom√°tica √© um ramo da intelig√™ncia artificial que permite aos computadores aprender a partir de dados sem serem explicitamente programados...\"\n",
    "    },\n",
    "    {\n",
    "        \"instrucao\": \"Como fazer um bolo de chocolate?\",\n",
    "        \"resposta\": \"Para fazer um bolo de chocolate: 1. Pr√©-aquece o forno a 180¬∞C. 2. Mistura 200g de farinha...\"\n",
    "    },\n",
    "    # ... mais exemplos\n",
    "]\n",
    "\n",
    "# Converter para formato de treino\n",
    "def formatar_prompt(exemplo):\n",
    "    \"\"\"Formato Alpaca\"\"\"\n",
    "    return f\"### Instru√ß√£o:\\n{exemplo['instrucao']}\\n\\n### Resposta:\\n{exemplo['resposta']}\"\n",
    "\n",
    "# Criar dataset\n",
    "dataset = Dataset.from_list(dados_treino)\n",
    "dataset = dataset.map(lambda x: {\"texto\": formatar_prompt(x)})\n",
    "\n",
    "print(f\"‚úì Dataset criado com {len(dataset)} exemplos\")\n",
    "```\n",
    "\n",
    "**Fine-tuning com LoRA (7B modelo):**\n",
    "\n",
    "```python\n",
    "# lora_finetuning.py\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "\n",
    "class FineTunerLoRA:\n",
    "    def __init__(self, modelo_base=\"meta-llama/Llama-2-7b-hf\"):\n",
    "        self.modelo_base = modelo_base\n",
    "        self.device = \"mps\"  # Apple Silicon\n",
    "        \n",
    "    def carregar_modelo_4bit(self):\n",
    "        \"\"\"\n",
    "        Carrega modelo em 4-bit para economizar mem√≥ria\n",
    "        \"\"\"\n",
    "        print(f\"üì• Carregando {self.modelo_base} em 4-bit...\")\n",
    "        \n",
    "        # Configura√ß√£o de quantiza√ß√£o\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        \n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        \n",
    "        # Carregar modelo\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.modelo_base,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Preparar para treino\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        \n",
    "        # Tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.modelo_base)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        print(\"‚úì Modelo carregado!\")\n",
    "        return model, tokenizer\n",
    "    \n",
    "    def configurar_lora(self, model):\n",
    "        \"\"\"\n",
    "        Adiciona adaptadores LoRA ao modelo\n",
    "        \"\"\"\n",
    "        print(\"üîß Configurando LoRA...\")\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            r=16,                    # Rank das matrizes LoRA (8-64)\n",
    "            lora_alpha=32,           # Scaling factor\n",
    "            target_modules=[\n",
    "                \"q_proj\",\n",
    "                \"k_proj\", \n",
    "                \"v_proj\",\n",
    "                \"o_proj\",\n",
    "                \"gate_proj\",\n",
    "                \"up_proj\",\n",
    "                \"down_proj\"\n",
    "            ],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        \n",
    "        model = get_peft_model(model, lora_config)\n",
    "        \n",
    "        # Estat√≠sticas\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        all_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        print(\"‚úì LoRA configurado!\")\n",
    "        print(f\"  Par√¢metros trein√°veis: {trainable_params:,} ({100*trainable_params/all_params:.2f}%)\")\n",
    "        print(f\"  Par√¢metros totais: {all_params:,}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def treinar(self, model, tokenizer, dataset, output_dir=\"./lora-model\"):\n",
    "        \"\"\"\n",
    "        Treina o modelo com LoRA\n",
    "        \"\"\"\n",
    "        print(\"üöÄ Iniciando treino...\")\n",
    "        \n",
    "        # Tokenizar dataset\n",
    "        def tokenize(examples):\n",
    "            return tokenizer(\n",
    "                examples[\"texto\"],\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "        \n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        \n",
    "        # Argumentos de treino otimizados para M1\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=4,      # Batch pequeno para M1\n",
    "            gradient_accumulation_steps=4,       # Simula batch 16\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,                           # Mixed precision\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"epoch\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            warmup_steps=100,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            optim=\"adamw_torch\",                 # Optimizador nativo PyTorch\n",
    "        )\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False\n",
    "        )\n",
    "        \n",
    "        # Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        # Treinar!\n",
    "        trainer.train()\n",
    "        \n",
    "        # Guardar\n",
    "        model.save_pretrained(output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        print(f\"‚úì Modelo guardado em {output_dir}\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "# Uso completo\n",
    "finetuner = FineTunerLoRA(\"meta-llama/Llama-2-7b-hf\")\n",
    "model, tokenizer = finetuner.carregar_modelo_4bit()\n",
    "model = finetuner.configurar_lora(model)\n",
    "\n",
    "# Carregar teu dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"dados_treino.json\")\n",
    "\n",
    "# Treinar\n",
    "model = finetuner.treinar(model, tokenizer, dataset[\"train\"])\n",
    "```\n",
    "\n",
    "#### Testar Modelo Fine-tuned\n",
    "\n",
    "```python\n",
    "# testar_modelo.py\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def testar_modelo_lora(base_model, lora_path, prompt):\n",
    "    \"\"\"\n",
    "    Testa modelo fine-tuned com LoRA\n",
    "    \"\"\"\n",
    "    # Carregar base\n",
    "    print(\"üì• Carregando modelo...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # Carregar adaptadores LoRA\n",
    "    model = PeftModel.from_pretrained(model, lora_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    \n",
    "    # Gerar resposta\n",
    "    print(f\"\\nü§ñ Prompt: {prompt}\\n\")\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"mps\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"üìù Resposta:\\n{resposta}\")\n",
    "    \n",
    "    return resposta\n",
    "\n",
    "# Teste\n",
    "prompt = \"### Instru√ß√£o:\\nExplica o que √© o chip M1 da Apple\\n\\n### Resposta:\"\n",
    "\n",
    "resposta = testar_modelo_lora(\n",
    "    base_model=\"meta-llama/Llama-2-7b-hf\",\n",
    "    lora_path=\"./lora-model\",\n",
    "    prompt=prompt\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 MLX Framework da Apple\n",
    "\n",
    "#### O que √© MLX?\n",
    "\n",
    "**MLX** = Machine Learning framework nativo da Apple para Apple Silicon\n",
    "- Desenvolvido pela Apple Research\n",
    "- Otimizado especificamente para M1/M2/M3\n",
    "- Unified Memory aproveitada ao m√°ximo\n",
    "- Sintaxe parecida com NumPy/PyTorch\n",
    "\n",
    "**Vantagens no M1:**\n",
    "- ‚úÖ Performance superior ao PyTorch MPS\n",
    "- ‚úÖ Menos uso de mem√≥ria\n",
    "- ‚úÖ Suporte nativo para quantiza√ß√£o\n",
    "- ‚úÖ LLMs otimizados (llama, mistral, phi)\n",
    "\n",
    "#### Instala√ß√£o e Setup\n",
    "\n",
    "```bash\n",
    "# Instalar MLX\n",
    "pip install mlx\n",
    "pip install mlx-lm  # Para modelos de linguagem\n",
    "\n",
    "# Verificar instala√ß√£o\n",
    "python -c \"import mlx.core as mx; print(mx.__version__)\"\n",
    "```\n",
    "\n",
    "#### Fine-tuning com MLX (Mais Eficiente!)\n",
    "\n",
    "```python\n",
    "# mlx_finetuning.py\n",
    "\"\"\"\n",
    "Fine-tuning com MLX - Mais r√°pido e eficiente que PyTorch no M1\n",
    "\"\"\"\n",
    "\n",
    "# Exemplo usando mlx-lm (wrapper de alto n√≠vel)\n",
    "from mlx_lm import load, generate\n",
    "from mlx_lm.tuner import train\n",
    "\n",
    "# 1. Carregar modelo base\n",
    "model, tokenizer = load(\"mlx-community/Mistral-7B-v0.1-4bit\")\n",
    "\n",
    "# 2. Preparar dados (formato JSONL)\n",
    "\"\"\"\n",
    "# dados.jsonl:\n",
    "# {\"text\": \"### Instru√ß√£o: ... ### Resposta: ...\"}\n",
    "# {\"text\": \"### Instru√ß√£o: ... ### Resposta: ...\"}\n",
    "\"\"\"\n",
    "\n",
    "# 3. Configura√ß√£o LoRA para MLX\n",
    "config = {\n",
    "    \"lora_layers\": 16,           # N√∫mero de camadas LoRA\n",
    "    \"lora_rank\": 16,             # Rank\n",
    "    \"lora_alpha\": 32,\n",
    "    \"batch_size\": 4,\n",
    "    \"iters\": 1000,               # Itera√ß√µes\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"steps_per_eval\": 100,\n",
    "    \"save_every\": 100,\n",
    "    \"adapter_file\": \"adapters.npz\"\n",
    "}\n",
    "\n",
    "# 4. Treinar\n",
    "print(\"üöÄ Treinando com MLX...\")\n",
    "train(\n",
    "    model=\"mlx-community/Mistral-7B-v0.1-4bit\",\n",
    "    data=\"dados.jsonl\",\n",
    "    valid_data=\"val.jsonl\",\n",
    "    **config\n",
    ")\n",
    "\n",
    "print(\"‚úì Treino completo! Adaptadores guardados em adapters.npz\")\n",
    "\n",
    "# 5. Testar\n",
    "prompt = \"### Instru√ß√£o: O que √© o MLX? ### Resposta:\"\n",
    "resposta = generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_tokens=200,\n",
    "    temp=0.7\n",
    ")\n",
    "print(resposta)\n",
    "```\n",
    "\n",
    "#### Converter Modelos para MLX\n",
    "\n",
    "```python\n",
    "# converter_para_mlx.py\n",
    "\"\"\"\n",
    "Converter modelos HuggingFace para formato MLX\n",
    "\"\"\"\n",
    "from mlx_lm import convert\n",
    "\n",
    "# Converter modelo\n",
    "convert(\n",
    "    hf_path=\"meta-llama/Llama-2-7b-hf\",  # Modelo HuggingFace\n",
    "    mlx_path=\"./llama-7b-mlx\",           # Destino MLX\n",
    "    quantize=True,                        # Quantizar para 4-bit\n",
    "    q_group_size=64,                      # Tamanho do grupo\n",
    "    q_bits=4                              # 4-bit quantization\n",
    ")\n",
    "\n",
    "print(\"‚úì Modelo convertido para MLX com quantiza√ß√£o 4-bit!\")\n",
    "```\n",
    "\n",
    "#### MLX vs PyTorch no M1 - Compara√ß√£o\n",
    "\n",
    "```python\n",
    "# benchmark_mlx_vs_pytorch.py\n",
    "import time\n",
    "import mlx.core as mx\n",
    "import torch\n",
    "\n",
    "def benchmark_inferencia():\n",
    "    \"\"\"\n",
    "    Compara velocidade MLX vs PyTorch\n",
    "    \"\"\"\n",
    "    # MLX\n",
    "    from mlx_lm import load, generate\n",
    "    model_mlx, tokenizer_mlx = load(\"mlx-community/Mistral-7B-v0.1-4bit\")\n",
    "    \n",
    "    prompt = \"Explica o que √© intelig√™ncia artificial\"\n",
    "    \n",
    "    # Teste MLX\n",
    "    start = time.time()\n",
    "    resposta_mlx = generate(model_mlx, tokenizer_mlx, prompt, max_tokens=100)\n",
    "    tempo_mlx = time.time() - start\n",
    "    \n",
    "    # PyTorch MPS\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    model_pt = AutoModelForCausalLM.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-v0.1\",\n",
    "        device_map=\"mps\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    tokenizer_pt = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "    \n",
    "    start = time.time()\n",
    "    inputs = tokenizer_pt(prompt, return_tensors=\"pt\").to(\"mps\")\n",
    "    outputs = model_pt.generate(**inputs, max_new_tokens=100)\n",
    "    tempo_pt = time.time() - start\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"BENCHMARK: MLX vs PyTorch MPS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"MLX:        {tempo_mlx:.2f}s\")\n",
    "    print(f\"PyTorch:    {tempo_pt:.2f}s\")\n",
    "    print(f\"Speedup:    {tempo_pt/tempo_mlx:.2f}x\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Executar\n",
    "benchmark_inferencia()\n",
    "\n",
    "# Resultado t√≠pico no M1 16GB:\n",
    "# MLX:        3.2s\n",
    "# PyTorch:    5.8s\n",
    "# Speedup:    1.8x ‚úÖ\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
