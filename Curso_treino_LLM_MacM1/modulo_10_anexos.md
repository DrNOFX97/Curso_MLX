# Anexos: ReferÃªncia RÃ¡pida

## Ãndice

### Anexo A: Comandos Ãšteis de Terminal
### Anexo B: Snippets de CÃ³digo ReutilizÃ¡veis
### Anexo C: Checklist de OptimizaÃ§Ã£o
### Anexo D: Recursos de Datasets Gratuitos
### Anexo E: GlossÃ¡rio de Termos
### Anexo F: Tabelas de ReferÃªncia RÃ¡pida

---

## Anexo A: Comandos Ãšteis de Terminal

### GestÃ£o de Ambiente

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONDA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Listar ambientes
conda env list

# Criar ambiente
conda create -n ml-m1 python=3.11 -y

# Activar/Desactivar
conda activate ml-m1
conda deactivate

# Exportar ambiente (para partilhar)
conda env export > environment.yml

# Criar de environment.yml
conda env create -f environment.yml

# Remover ambiente
conda env remove -n ml-m1

# Actualizar conda
conda update -n base conda

# Limpar cache (liberta espaÃ§o)
conda clean --all


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PIP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Listar pacotes instalados
pip list

# Instalar versÃ£o especÃ­fica
pip install tensorflow==2.16.1

# Actualizar pacote
pip install --upgrade transformers

# Guardar dependÃªncias
pip freeze > requirements.txt

# Instalar de requirements.txt
pip install -r requirements.txt

# Desinstalar
pip uninstall tensorflow

# Ver informaÃ§Ã£o de pacote
pip show tensorflow

# Procurar pacote
pip search "machine learning"
```

### MonitorizaÃ§Ã£o do Sistema

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MEMÃ“RIA E CPU
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Uso de memÃ³ria (simples)
vm_stat

# Uso de memÃ³ria (detalhado)
top -l 1 | head -n 10

# Monitorizar em tempo real
htop  # Instalar: brew install htop

# Ver processos Python
ps aux | grep python

# Matar processo
kill -9 <PID>

# Ver espaÃ§o em disco
df -h

# Ver tamanho de pasta
du -sh models/

# Ver ficheiros grandes
du -ah . | sort -rh | head -n 20


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GPU/METAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Ver actividade GPU (Activity Monitor)
open -a "Activity Monitor"

# Stats do sistema
system_profiler SPHardwareDataType

# Temperatura e ventoinhas (requer iStat Menus ou similar)
# Alternativa: usar Activity Monitor â†’ Window â†’ GPU History


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REDE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Testar velocidade download
curl -o /dev/null http://speedtest.wdc01.softlayer.com/downloads/test100.zip

# Ver processos que usam rede
lsof -i

# Download com progresso
curl -L -o model.zip https://exemplo.com/model.zip \
  --progress-bar
```

### Git Essenciais

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GIT BASICS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Inicializar repo
git init

# Clonar repo
git clone https://github.com/user/repo.git

# Status
git status

# Adicionar ficheiros
git add .
git add src/  # apenas pasta src

# Commit
git commit -m "Adiciona modelo de classificaÃ§Ã£o"

# Push
git push origin main

# Pull
git pull origin main

# Ver histÃ³rico
git log --oneline --graph

# Ver diferenÃ§as
git diff


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GIT LFS (para modelos grandes)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Instalar
brew install git-lfs
git lfs install

# Trackear ficheiros grandes
git lfs track "*.keras"
git lfs track "*.h5"
git lfs track "*.pth"

# Adicionar .gitattributes
git add .gitattributes
git commit -m "Adiciona LFS tracking"

# Ver ficheiros tracked
git lfs ls-files
```

### Jupyter Notebook

```bash
# Iniciar Jupyter
jupyter notebook

# Jupyter Lab (interface moderna)
jupyter lab

# Listar kernels
jupyter kernelspec list

# Adicionar ambiente conda como kernel
python -m ipykernel install --user --name=ml-m1

# Remover kernel
jupyter kernelspec uninstall ml-m1

# Converter notebook para script
jupyter nbconvert --to script notebook.ipynb

# Converter para HTML
jupyter nbconvert --to html notebook.ipynb
```

### Scripts Ãšteis One-liners

```bash
# Encontrar ficheiros Python modificados hoje
find . -name "*.py" -mtime -1

# Contar linhas de cÃ³digo Python
find . -name "*.py" | xargs wc -l

# Apagar todos os __pycache__
find . -type d -name __pycache__ -exec rm -r {} +

# Apagar checkpoints Jupyter
find . -name ".ipynb_checkpoints" -type d -exec rm -rf {} +

# Ver tamanho de cada pasta (top 10)
du -sh */ | sort -rh | head -10

# Comprimir modelo
tar -czf modelo.tar.gz models/

# Descomprimir
tar -xzf modelo.tar.gz

# Sync de pasta (backup)
rsync -avz --progress models/ backup/models/
```

---

## Anexo B: Snippets de CÃ³digo ReutilizÃ¡veis

### Setup BÃ¡sico

```python
# setup.py
"""
ConfiguraÃ§Ã£o padrÃ£o para qualquer projecto ML no M1
Copiar para inÃ­cio de cada script
"""
import os
import random
import numpy as np
import tensorflow as tf

# Seed para reprodutibilidade
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'

set_seed(42)

# Mixed precision
policy = tf.keras.mixed_precision.Policy('mixed_float16')
tf.keras.mixed_precision.set_global_policy(policy)

# Verificar GPU
gpus = tf.config.list_physical_devices('GPU')
print(f"GPUs disponÃ­veis: {len(gpus)}")
if gpus:
    print(f"âœ“ GPU detectada: {gpus[0].name}")
else:
    print("âš ï¸  GPU nÃ£o detectada!")

# Imports comuns
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import pandas as pd

print("âœ“ Setup completo!")
```

### Data Augmentation Templates

```python
# augmentation_templates.py

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IMAGENS - Augmentation Leve (Documentos, texto)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
aug_leve = keras.Sequential([
    layers.RandomRotation(0.05),      # Â±5%
    layers.RandomTranslation(0.05, 0.05),
    layers.RandomBrightness(0.1),
], name="aug_leve")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IMAGENS - Augmentation MÃ©dio (Fotografia geral)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
aug_medio = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.15),
    layers.RandomZoom(0.15),
    layers.RandomTranslation(0.1, 0.1),
    layers.RandomContrast(0.2),
], name="aug_medio")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IMAGENS - Augmentation Forte (Datasets pequenos)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
aug_forte = keras.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.3),
    layers.RandomZoom(0.3),
    layers.RandomTranslation(0.2, 0.2),
    layers.RandomContrast(0.3),
    layers.RandomBrightness(0.2),
], name="aug_forte")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TEXTO - Augmentation (Back-translation, synonims)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def text_augmentation(text, prob=0.3):
    """Augmentation simples de texto"""
    import random
    
    words = text.split()
    
    # Shuffle aleatÃ³rio de palavras (30% chance)
    if random.random() < prob:
        random.shuffle(words)
    
    # Duplicar palavras aleatÃ³rias (30% chance)
    if random.random() < prob and len(words) > 3:
        idx = random.randint(0, len(words)-1)
        words.insert(idx, words[idx])
    
    return ' '.join(words)
```

### Callbacks Standard

```python
# callbacks_standard.py

def get_callbacks(
    model_name="model",
    monitor='val_accuracy',
    patience_early=10,
    patience_lr=5
):
    """
    Callbacks padrÃ£o para qualquer treino
    
    Returns:
        List de callbacks prontos a usar
    """
    callbacks = [
        # Early stopping
        keras.callbacks.EarlyStopping(
            monitor=monitor,
            patience=patience_early,
            restore_best_weights=True,
            verbose=1
        ),
        
        # Reduce LR on plateau
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=patience_lr,
            min_lr=1e-7,
            verbose=1
        ),
        
        # Model checkpoint
        keras.callbacks.ModelCheckpoint(
            f'checkpoints/{model_name}_best.keras',
            monitor=monitor,
            save_best_only=True,
            verbose=1
        ),
        
        # TensorBoard
        keras.callbacks.TensorBoard(
            log_dir=f'logs/{model_name}',
            histogram_freq=1
        ),
        
        # CSV Logger (backup)
        keras.callbacks.CSVLogger(
            f'logs/{model_name}_history.csv'
        )
    ]
    
    return callbacks

# Uso:
# callbacks = get_callbacks("classificador_v1")
# model.fit(..., callbacks=callbacks)
```

### Data Loading Templates

```python
# data_loaders.py

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Carregar Imagens de DirectÃ³rio
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_image_dataset(
    data_dir,
    img_size=(224, 224),
    batch_size=32,
    validation_split=0.2
):
    """Template standard para datasets de imagens"""
    
    train_ds = keras.preprocessing.image_dataset_from_directory(
        data_dir,
        validation_split=validation_split,
        subset="training",
        seed=42,
        image_size=img_size,
        batch_size=batch_size,
        shuffle=True
    )
    
    val_ds = keras.preprocessing.image_dataset_from_directory(
        data_dir,
        validation_split=validation_split,
        subset="validation",
        seed=42,
        image_size=img_size,
        batch_size=batch_size,
        shuffle=False
    )
    
    # OptimizaÃ§Ãµes
    train_ds = train_ds.prefetch(tf.data.AUTOTUNE)
    val_ds = val_ds.prefetch(tf.data.AUTOTUNE)
    
    return train_ds, val_ds


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Carregar CSV
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_csv_dataset(
    csv_path,
    text_col,
    label_col,
    test_size=0.2
):
    """Template para datasets tabulares/texto"""
    import pandas as pd
    from sklearn.model_selection import train_test_split
    
    df = pd.read_csv(csv_path)
    
    train_df, val_df = train_test_split(
        df,
        test_size=test_size,
        stratify=df[label_col],
        random_state=42
    )
    
    return train_df, val_df


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TFRecord (grandes datasets)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_tfrecord_dataset(
    tfrecord_path,
    batch_size=32
):
    """Para datasets optimizados"""
    
    dataset = tf.data.TFRecordDataset(tfrecord_path)
    
    def parse_fn(example):
        # Define o teu schema aqui
        features = {
            'image': tf.io.FixedLenFeature([], tf.string),
            'label': tf.io.FixedLenFeature([], tf.int64),
        }
        parsed = tf.io.parse_single_example(example, features)
        image = tf.io.decode_jpeg(parsed['image'])
        label = parsed['label']
        return image, label
    
    dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    
    return dataset
```

### VisualizaÃ§Ã£o

```python
# visualization.py

def plot_training_curves(history, save_path='training_curves.png'):
    """Plot bonito de curvas de treino"""
    import matplotlib.pyplot as plt
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    # Accuracy
    ax1.plot(history.history['accuracy'], label='Train', linewidth=2)
    ax1.plot(history.history['val_accuracy'], label='Validation', linewidth=2)
    ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Accuracy')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Loss
    ax2.plot(history.history['loss'], label='Train', linewidth=2)
    ax2.plot(history.history['val_loss'], label='Validation', linewidth=2)
    ax2.set_title('Model Loss', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Loss')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ GrÃ¡fico guardado: {save_path}")


def plot_confusion_matrix(y_true, y_pred, labels, save_path='confusion_matrix.png'):
    """Matriz de confusÃ£o bonita"""
    from sklearn.metrics import confusion_matrix
    import seaborn as sns
    import matplotlib.pyplot as plt
    
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(
        cm, 
        annot=True, 
        fmt='d', 
        cmap='Blues',
        xticklabels=labels,
        yticklabels=labels,
        cbar_kws={'label': 'Count'}
    )
    plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ Matriz guardada: {save_path}")


def plot_samples(dataset, class_names, num_samples=9):
    """Mostra amostras do dataset"""
    import matplotlib.pyplot as plt
    
    plt.figure(figsize=(12, 12))
    
    for images, labels in dataset.take(1):
        for i in range(min(num_samples, len(images))):
            ax = plt.subplot(3, 3, i + 1)
            plt.imshow(images[i].numpy().astype("uint8"))
            plt.title(class_names[labels[i]])
            plt.axis("off")
    
    plt.tight_layout()
    plt.show()
```

---

## Anexo C: Checklist de OptimizaÃ§Ã£o

### Antes do Treino

```markdown
## ğŸ” CHECKLIST PRÃ‰-TREINO

### Ambiente
- [ ] Python ARM64 verificado (`platform.machine()` â†’ arm64)
- [ ] Ambiente virtual activado
- [ ] Todas as dependÃªncias instaladas
- [ ] VersÃµes correctas (TF 2.16.x + Metal 1.1.x)

### Dados
- [ ] Dataset verificado e balanceado
- [ ] Split train/val/test correcto
- [ ] Sem data leakage
- [ ] Augmentation configurado
- [ ] Pipeline optimizado (prefetch, cache se aplicÃ¡vel)

### Modelo
- [ ] Arquitectura apropriada para tarefa
- [ ] Tamanho adequado para M1 16GB
- [ ] Pesos prÃ©-treinados carregados (se aplicÃ¡vel)
- [ ] Mixed precision activado

### ConfiguraÃ§Ã£o
- [ ] Batch size testado e optimizado
- [ ] Learning rate apropriado
- [ ] Callbacks configurados (Early Stop, ReduceLR)
- [ ] Checkpointing activo
- [ ] Seed definido (reprodutibilidade)

### Sistema
- [ ] MemÃ³ria disponÃ­vel >8GB
- [ ] Outras apps fechadas
- [ ] GPU/Metal verificado
- [ ] EspaÃ§o em disco >10GB livre
```

### Durante o Treino

```markdown
## âš™ï¸ MONITORIZAÃ‡ÃƒO ACTIVA

### A Cada Epoch
- [ ] Val accuracy a subir?
- [ ] Gap train-val <15%?
- [ ] Loss a descer consistentemente?
- [ ] MemÃ³ria <90%?

### Sinais de Problema
- [ ] âš ï¸ Val accuracy estÃ¡vel mas train sobe â†’ Overfitting
- [ ] âš ï¸ Ambas estÃ¡veis e baixas â†’ Underfitting
- [ ] âš ï¸ Loss explode (NaN) â†’ LR muito alto
- [ ] âš ï¸ MemÃ³ria a crescer â†’ Memory leak
- [ ] âš ï¸ Muito lento â†’ Pipeline de dados lento

### AcÃ§Ãµes Correctivas
- [ ] Se overfitting â†’ Adiciona regularizaÃ§Ã£o/dropout
- [ ] Se underfitting â†’ Modelo maior ou mais epochs
- [ ] Se OOM â†’ Reduz batch_size
- [ ] Se lento â†’ Verifica prefetch/paralelizaÃ§Ã£o
```

### ApÃ³s Treino

```markdown
## âœ… PÃ“S-TREINO

### AvaliaÃ§Ã£o
- [ ] Testado em test set (nÃ£o usado no treino)
- [ ] MÃ©tricas documentadas
- [ ] Confusion matrix analisada
- [ ] Erros comuns identificados

### OptimizaÃ§Ã£o
- [ ] QuantizaÃ§Ã£o testada (se deployment)
- [ ] Pruning considerado
- [ ] Tamanho final aceitÃ¡vel?
- [ ] Velocidade de inferÃªncia aceitÃ¡vel?

### DocumentaÃ§Ã£o
- [ ] Modelo versionado com metadata
- [ ] HiperparÃ¢metros guardados
- [ ] README actualizado
- [ ] Exemplos de uso criados

### PrÃ³ximos Passos
- [ ] Deploy planeado
- [ ] Melhorias identificadas
- [ ] Baseline estabelecida para futuras versÃµes
```

---

## Anexo D: Recursos de Datasets Gratuitos

### Imagens

```markdown
## ğŸ–¼ï¸ DATASETS DE IMAGENS

### ClassificaÃ§Ã£o Geral
ğŸ”— ImageNet (via Kaggle)
   - 1.2M imagens, 1000 classes
   - Benchmark standard
   - kaggle.com/c/imagenet-object-localization-challenge

ğŸ”— CIFAR-10 / CIFAR-100
   - 60K imagens pequenas (32x32)
   - 10/100 classes
   - DisponÃ­vel via tf.keras.datasets

ğŸ”— Fashion-MNIST
   - 70K imagens de roupa (28x28)
   - 10 classes
   - DisponÃ­vel via tf.keras.datasets

### EspecÃ­ficos
ğŸ”— Plantas (PlantVillage)
   - 54K imagens de doenÃ§as em plantas
   - kaggle.com/datasets/emmarex/plantdisease

ğŸ”— Animais (Animals-10)
   - 28K imagens, 10 animais
   - kaggle.com/datasets/alessiocorrado99/animals10

ğŸ”— Faces (CelebA)
   - 200K faces de celebridades
   - AnotaÃ§Ãµes de atributos
   - kaggle.com/datasets/jessicali9530/celeba-dataset

ğŸ”— MÃ©dico (Chest X-Ray)
   - Raios-X de tÃ³rax
   - Pneumonia detection
   - kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia

### Portugueses
ğŸ”— Azulejos Portugueses
   - github.com/delftrobotics/azulejos-dataset
   
ğŸ”— Vinhos Portugueses (dados tabulares)
   - archive.ics.uci.edu/ml/datasets/wine+quality
```

### Texto/NLP

```markdown
## ğŸ“ DATASETS DE TEXTO

### PortuguÃªs
ğŸ”— ASSIN (Similaridade SemÃ¢ntica)
   - Pares de frases PT-BR e PT-PT
   - kaggle.com/datasets/assin

ğŸ”— Reviews Produtos (PT-BR)
   - Amazon reviews em portuguÃªs
   - kaggle.com/datasets/fredericods/ptbr-sentiment-analysis-datasets

ğŸ”— NotÃ­cias Portuguesas
   - PÃºblico, Observador, etc.
   - Usar web scraping (respeitar ToS)

### InglÃªs
ğŸ”— IMDB Reviews
   - 50K reviews de filmes
   - DisponÃ­vel via tf.keras.datasets

ğŸ”— AG News
   - 120K notÃ­cias, 4 categorias
   - huggingface.co/datasets/ag_news

ğŸ”— Wikipedia
   - Dumps completos
   - dumps.wikimedia.org

ğŸ”— Common Crawl
   - Petabytes de texto web
   - commoncrawl.org

### Multilingual
ğŸ”— OSCAR
   - Corpus web multilingual
   - Inclui portuguÃªs
   - huggingface.co/datasets/oscar
```

### Ãudio

```markdown
## ğŸµ DATASETS DE ÃUDIO

ğŸ”— Speech Commands
   - Comandos de voz curtos
   - tensorflow.org/datasets/catalog/speech_commands

ğŸ”— Common Voice (Mozilla)
   - Voz em mÃºltiplas lÃ­nguas
   - Inclui portuguÃªs
   - commonvoice.mozilla.org

ğŸ”— LibriSpeech
   - 1000h de audiolivros
   - openslr.org/12
```

### Tabulares

```markdown
## ğŸ“Š DATASETS TABULARES

ğŸ”— UCI Machine Learning Repository
   - Centenas de datasets
   - archive.ics.uci.edu/ml

ğŸ”— Kaggle Datasets
   - Milhares de datasets
   - kaggle.com/datasets

ğŸ”— Google Dataset Search
   - Motor de busca de datasets
   - datasetsearch.research.google.com

ğŸ”— Papers With Code Datasets
   - Datasets de papers
   - paperswithcode.com/datasets
```

### Onde Procurar

```markdown
## ğŸ” MOTORES DE BUSCA

1. Kaggle (kaggle.com/datasets)
   âœ… Muito datasets prontos
   âœ… Notebooks de exemplo
   âœ… APIs para download

2. Hugging Face (huggingface.co/datasets)
   âœ… Foco em NLP
   âœ… FÃ¡cil integraÃ§Ã£o
   âœ… Streaming de grandes datasets

3. TensorFlow Datasets (tensorflow.org/datasets)
   âœ… Prontos para usar
   âœ… Pipeline tf.data
   âœ… Bem documentados

4. Papers With Code
   âœ… Datasets de research
   âœ… Benchmarks incluÃ­dos

5. GitHub (github.com/topics/dataset)
   âœ… Datasets especializados
   âœ… Scripts de processamento
```

---

## Anexo E: GlossÃ¡rio de Termos

```markdown
## ğŸ“– GLOSSÃRIO ML/DL

### A
**Accuracy** - Percentagem de previsÃµes correctas
**Activation Function** - FunÃ§Ã£o nÃ£o-linear (ReLU, sigmoid, etc.)
**Adam** - Optimizador adaptativo popular
**Augmentation** - CriaÃ§Ã£o de variaÃ§Ãµes de dados para treino

### B
**Backpropagation** - Algoritmo para calcular gradientes
**Batch** - Conjunto de exemplos processados juntos
**Batch Size** - NÃºmero de exemplos por batch
**Bias** - ParÃ¢metro adicional nos neurÃ³nios / Enviesamento nos dados

### C
**Checkpoint** - Snapshot do modelo durante treino
**CNN** - Convolutional Neural Network
**Confusion Matrix** - Tabela de previsÃµes vs realidade

### D
**Dataset** - Conjunto de dados
**Dropout** - TÃ©cnica de regularizaÃ§Ã£o
**Dense Layer** - Camada totalmente conectada

### E
**Early Stopping** - Para treino quando nÃ£o melhora
**Embedding** - RepresentaÃ§Ã£o vectorial de dados
**Epoch** - Uma passagem completa pelo dataset

### F
**Fine-tuning** - Ajuste fino de modelo prÃ©-treinado
**FP16** - Float16, mixed precision
**Frozen Layers** - Camadas nÃ£o-treinÃ¡veis

### G
**GPU** - Graphics Processing Unit
**Gradient** - Derivada da loss em relaÃ§Ã£o aos parÃ¢metros
**Gradient Descent** - Algoritmo de optimizaÃ§Ã£o

### H
**Hyperparameter** - ParÃ¢metro de configuraÃ§Ã£o (LR, batch size)
**Hidden Layer** - Camada intermediÃ¡ria da rede

### I
**Inference** - Usar modelo treinado para previsÃµes
**Input Shape** - DimensÃµes do input

### L
**Label** - Valor verdadeiro / target
**Learning Rate (LR)** - Tamanho do passo na optimizaÃ§Ã£o
**Loss** - FunÃ§Ã£o que mede erro do modelo
**LoRA** - Low-Rank Adaptation (fine-tuning eficiente)

### M
**MPS** - Metal Performance Shaders (GPU do M1)
**Mixed Precision** - Treino com FP16+FP32

### N
**Neuron** - Unidade bÃ¡sica de rede neural
**Normalization** - Escalar dados para range padrÃ£o

### O
**OOM** - Out Of Memory
**Overfitting** - Modelo decora treino, nÃ£o generaliza
**Optimizer** - Algoritmo que actualiza pesos (Adam, SGD)

### P
**Parameter** - Peso ou bias aprendido
**Pooling** - ReduÃ§Ã£o de dimensionalidade
**Preprocessing** - PreparaÃ§Ã£o de dados

### Q
**Quantization** - ReduÃ§Ã£o de precisÃ£o (FP32â†’INT8)

### R
**ReLU** - Rectified Linear Unit (activaÃ§Ã£o)
**Regularization** - TÃ©cnicas para prevenir overfitting

### S
**SGD** - Stochastic Gradient Descent
**Softmax** - FunÃ§Ã£o para probabilidades de classes
**Split** - DivisÃ£o de dados (train/val/test)

### T
**Tensor** - Array multidimensional
**Transfer Learning** - Usar modelo prÃ©-treinado
**Training Loop** - Ciclo de treino (forward/backward)

### U
**Underfitting** - Modelo nÃ£o aprende o suficiente
**UMA** - Unified Memory Architecture (M1)

### V
**Validation Set** - Dados para avaliar durante treino

### W
**Weight** - ParÃ¢metro aprendido
**Weight Decay** - RegularizaÃ§Ã£o L2
```

---

## Anexo F: Tabelas de ReferÃªncia RÃ¡pida

### Batch Sizes Recomendados (M1 16GB)

| Modelo | Input Size | Batch Size | MemÃ³ria ~|
|--------|-----------|------------|----------|
| MobileNetV2 | 224x224 | 64 | 6GB |
| EfficientNetB0 | 224x224 | 32-64 | 7GB |
| ResNet50 | 224x224 | 32 | 8GB |
| EfficientNetB3 | 300x300 | 16 | 9GB |
| ViT-Base | 224x224 | 16 | 10GB |
| DistilBERT | 128 tokens | 16 | 8GB |
| BERT-base | 128 tokens | 8 | 10GB |
| LLaMA 7B (4-bit) | - | 2 | 6GB |

### Learning Rates TÃ­picos

| SituaÃ§Ã£o | Learning Rate | Notas |
|----------|--------------|-------|
| **Treino from scratch** | 1e-3 | PadrÃ£o para Adam |
| **Transfer learning (fase 1)** | 1e-3 | SÃ³ classificador |
| **Fine-tuning (fase 2)** | 1e-5 a 1e-4 | Toda a rede |
| **BERT fine-tuning** | 2e-5 a 5e-5 | Standard |
| **LoRA LLM** | 1e-4 a 3e-4 | Mais alto que full |
| **Se loss
