## M√≥dulo 5: Modelos de Linguagem Grandes (LLMs)

### 5.1 Limita√ß√µes e Estrat√©gias

#### Por que 16GB √© Limitante para LLMs?

**Requisitos t√≠picos de mem√≥ria para LLMs:**

| Modelo | Par√¢metros | FP32 | FP16 | INT8 | INT4 |
|--------|-----------|------|------|------|------|
| GPT-2 Small | 124M | 500MB | 250MB | 125MB | 63MB ‚úÖ |
| GPT-2 Medium | 355M | 1.4GB | 700MB | 350MB | 175MB ‚úÖ |
| LLaMA-2 7B | 7B | 28GB ‚ùå | 14GB ‚ö†Ô∏è | 7GB ‚úÖ | 3.5GB ‚úÖ |
| LLaMA-2 13B | 13B | 52GB ‚ùå | 26GB ‚ùå | 13GB ‚ö†Ô∏è | 6.5GB ‚úÖ |
| Mistral 7B | 7B | 28GB ‚ùå | 14GB ‚ö†Ô∏è | 7GB ‚úÖ | 3.5GB ‚úÖ |

**Realidade no M1 16GB:**
- Sistema operativo: ~3-4GB
- Apps em background: ~1-2GB
- **Dispon√≠vel para modelo: ~10-12GB**

**Conclus√£o:**
- ‚úÖ Modelos at√© 7B: Vi√°veis com quantiza√ß√£o INT4/INT8
- ‚ö†Ô∏è Modelos 13B: Poss√≠vel apenas com INT4 + otimiza√ß√µes
- ‚ùå Modelos 30B+: Imposs√≠vel carregar completos na mem√≥ria

#### Estrat√©gias para Trabalhar com LLMs no M1

**1. Quantiza√ß√£o Extrema (4-bit, GGUF)**

GGUF = GPT-Generated Unified Format (sucessor de GGML)
- Formato otimizado para infer√™ncia em CPU/GPU unificada
- Quantiza√ß√£o 4-bit com grupos (mant√©m qualidade)
- Ideal para Apple Silicon

**2. LoRA (Low-Rank Adaptation)**

Em vez de treinar todos os par√¢metros, adiciona matrizes pequenas:
```
Modelo original:  7B par√¢metros (frozen)
     +
LoRA adapters:    ~10M par√¢metros (trein√°veis)
     =
Modelo fine-tuned mantendo 99.9% frozen!
```

**Economia de mem√≥ria:**
- Treino normal de 7B: ~40GB necess√°rios (gradientes + optimizador)
- Treino com LoRA: ~12GB necess√°rios ‚úÖ

**3. QLoRA (Quantized LoRA)**

Combina quantiza√ß√£o + LoRA:
```
Base model em INT4:  3.5GB
LoRA params em FP16: 20MB
Gradientes:          ~2GB
Total:               ~6GB ‚úÖ Cabe no M1!
```

#### Formato GGUF - Explica√ß√£o T√©cnica

```python
# compreender_gguf.py
"""
GGUF (GPT-Generated Unified Format)

Caracter√≠sticas:
- Formato bin√°rio otimizado
- Suporta quantiza√ß√£o 2-bit at√© 8-bit
- Metadados inclu√≠dos (arquitetura, vocabul√°rio)
- Carregamento r√°pido (mmap)
- Ideal para llama.cpp e MLX
"""

# Tipos de quantiza√ß√£o GGUF:
quantizacoes = {
    'Q2_K': '2-bit, muito compacto, perda qualidade',
    'Q3_K_S': '3-bit small, bom compromisso',
    'Q4_0': '4-bit, recomendado geral',
    'Q4_K_M': '4-bit medium, melhor qualidade',
    'Q5_0': '5-bit, alta qualidade',
    'Q5_K_M': '5-bit medium, muito boa',
    'Q8_0': '8-bit, quase sem perda',
}

# Escolha para M1 16GB:
# - Q4_K_M: Melhor equil√≠brio qualidade/tamanho
# - Q5_K_M: Se tiveres espa√ßo extra
# - Q8_0: Apenas modelos pequenos (<3B)
```

---

### 5.2 Fine-tuning de LLMs Pequenos

#### Cuidados com Datasets e Formatos Ideais

**‚ö†Ô∏è CR√çTICO: A qualidade do dataset √© mais importante que a quantidade!**

#### Princ√≠pios Fundamentais

**1. Qualidade sobre Quantidade**

```python
# Exemplo de dataset de baixa qualidade:
# Respostas gen√©ricas, erros, etc.

# Exemplo de dataset de alta qualidade:
# Respostas precisas, bem escritas, diversas.
```

**2. Diversidade √© Essencial**

```python
# analisar_diversidade.py
from collections import Counter
import numpy as np

def analisar_diversidade(dataset):
    """
    Analisa diversos aspetos do dataset
    """
    # ... (c√≥digo de an√°lise de comprimento, palavras iniciais, t√≥picos, duplicados)
    pass

# Se uma palavra aparece >30%, h√° falta de diversidade!
# Se categorias est√£o desbalanceadas, usar balancear_categorias()
```

**3. Balanceamento**

```python
# balancear_dataset.py
from collections import Counter
import random

def balancear_categorias(dataset, max_por_categoria=100):
    """
    Balanceia dataset para evitar overfitting em categorias sobre-representadas
    """
    # ... (c√≥digo para agrupar por categoria e amostrar)
    pass

# Uso
dataset_balanceado = balancear_categorias(dataset, max_por_categoria=200)
```

#### Formatos de Dataset Ideais

**Formato 1: Alpaca (Mais Comum)**

```json
{
  "instruction": "Explica o que √© aprendizagem autom√°tica",
  "input": "",
  "output": "Aprendizagem autom√°tica √© um ramo da intelig√™ncia artificial que permite aos computadores aprender a partir de dados sem serem explicitamente programados..."
}
```

**Quando usar:**
- Fine-tuning de instru√ß√µes
- Q&A geral
- Tarefas de seguir comandos

**Formato em texto:**

```python
# formato_alpaca.py
def formatar_prompt(instrucao, resposta, contexto=""):
    """Formato Alpaca padr√£o"""
    if contexto:
        prompt = f"### Instru√ß√£o:\n{instrucao}\n\n### Contexto:\n{contexto}\n\n### Resposta:\n{resposta}" 
    else:
        prompt = f"### Instru√ß√£o:\n{instrucao}\n\n### Resposta:\n{resposta}"
    
    return prompt

# Exemplo
exemplo = formatar_prompt(
    instrucao="Traduz para ingl√™s: Ol√°, como est√°s?",
    resposta="Hello, how are you?"
)
print(exemplo)
```

**Formato 2: ChatML (Para Conversas)**

```json
{
  "messages": [
    {"role": "system", "content": "√âs um assistente prest√°vel em portugu√™s."},
    {"role": "user", "content": "Como fazer um bolo?"},
    {"role": "assistant", "content": "Para fazer um bolo: 1. ..."}
  ]
}
```

**Quando usar:**
- Conversas multi-turn
- Assistentes com personalidade
- Contexto de sistema importante

**Implementa√ß√£o:**

```python
# formato_chatml.py
def formatar_chatml(mensagens):
    """
    Formato ChatML (usado por GPT, Mistral, etc.)
    """
    prompt = ""
    
    for msg in mensagens:
        role = msg['role']
        content = msg['content']
        
        if role == "system":
            prompt += f"<|im_start|>system\n{content}<|im_end|>\n"
        elif role == "user":
            prompt += f"<|im_start|>user\n{content}<|im_end|>\n"
        elif role == "assistant":
            prompt += f"<|im_start|>assistant\n{content}<|im_end|>\n"
    
    return prompt

# Exemplo de conversa
conversa = [
    {"role": "system", "content": "√âs um tutor de matem√°tica."},
    {"role": "user", "content": "Quanto √© 2+2?"},
    {"role": "assistant", "content": "2+2 = 4"},
    {"role": "user", "content": "E 5*3?"},
    {"role": "assistant", "content": "5*3 = 15"}
]

prompt = formatar_chatml(conversa)
```

**Formato 3: Completion (Mais Simples)**

```json
{
  "text": "Pergunta: O que √© Python?\n\nResposta: Python √© uma linguagem..."
}
```

**Quando usar:**
- Datasets muito simples
- Continuar texto
- Estilo liter√°rio

#### Tamanho Ideal do Dataset

**Regras gerais:**

| Tipo de Fine-tuning | M√≠nimo | Recomendado | Ideal |
|---------------------|--------|-------------|-------|
| **Adapta√ß√£o de estilo** | 50 | 200 | 500 |
| **Dom√≠nio espec√≠fico** | 200 | 1.000 | 5.000 |
| **Novo conhecimento** | 1.000 | 5.000 | 10.000+ |
| **Mudan√ßa de comportamento** | 500 | 2.000 | 5.000 |

**‚ö†Ô∏è IMPORTANTE: Com LoRA no M1, datasets grandes (>5k) podem ser problem√°ticos**

```python
# calcular_tempo_treino.py
def estimar_tempo_treino(num_exemplos, epochs=3, batch_size=4):
    """
    Estima tempo de treino no M1 16GB
    """
    steps_por_epoch = num_exemplos // batch_size
    total_steps = steps_por_epoch * epochs
    
    # M1 processa ~2-3 steps/segundo para LLaMA 7B com LoRA
    tempo_segundos = total_steps / 2.5
    
    horas = tempo_segundos / 3600
    
    print(f"üìä Estimativa de Treino")
    print(f"  Exemplos: {num_exemplos}")
    print(f"  Epochs: {epochs}")
    print(f"  Batch size: {batch_size}")
    print(f"  Total steps: {total_steps}")
    print(f"  Tempo estimado: {horas:.1f} horas")
    
    if horas > 12:
        print(f"\n‚ö†Ô∏è  Considera reduzir dataset ou epochs!")

# Testa
estimar_tempo_treino(num_exemplos=5000, epochs=3, batch_size=4)
```

#### Cuidados Cr√≠ticos

**1. Evitar Data Leakage**

```python
# split_correto.py
from sklearn.model_selection import train_test_split

def split_dataset_correto(dataset):
    """
    Split correto para evitar leakage
    """
    # ‚ùå ERRADO: Split aleat√≥rio se houver conversas multi-turn
    # Uma parte da conversa no treino, outra no teste = leakage!
    
    # ‚úÖ CORRETO: Split por "conversa_id" ou "user_id"
    
    # Agrupar por conversa
    conversas = {}
    for exemplo in dataset:
        conv_id = exemplo.get('conversa_id', exemplo.get('id'))
        if conv_id not in conversas:
            conversas[conv_id] = []
        conversas[conv_id].append(exemplo)
    
    # Split por conversas completas
    conv_ids = list(conversas.keys())
    train_ids, test_ids = train_test_split(conv_ids, test_size=0.2, random_state=42)
    
    train_data = []
    test_data = []
    
    for conv_id in train_ids:
        train_data.extend(conversas[conv_id])
    
    for conv_id in test_ids:
        test_data.extend(conversas[conv_id])
    
    print(f"‚úì Split correto:")
    print(f"  Treino: {len(train_data)} exemplos ({len(train_ids)} conversas)")
    print(f"  Teste:  {len(test_data)} exemplos ({len(test_ids)} conversas)")
    
    return train_data, test_data
```

**2. Limpeza de Dados**

```python
# limpar_dataset.py
import re
import unicodedata

def limpar_texto(texto):
    """
    Limpa texto removendo problemas comuns
    """
    # Normalizar unicode
    texto = unicodedata.normalize('NFKC', texto)
    
    # Remover espa√ßos m√∫ltiplos
    texto = re.sub(r'\s+', ' ', texto)
    
    # Remover caracteres de controlo (excepto \n)
    texto = ''.join(char for char in texto if char == '\n' or not unicodedata.category(char).startswith('C'))
    
    # Remover linhas vazias m√∫ltiplas
    texto = re.sub(r'\n\s*\n', '\n\n', texto)
    
    # Trim
    texto = texto.strip()
    
    return texto

def validar_exemplo(exemplo, min_length=10, max_length=2048):
    """
    Valida se exemplo √© adequado
    """
    instrucao = exemplo.get('instrucao', '')
    resposta = exemplo.get('resposta', '')
    
    # Verifica√ß√µes
    if len(resposta) < min_length:
        return False, "Resposta muito curta"
    
    if len(resposta) > max_length:
        return False, "Resposta muito longa"
    
    if not instrucao or not resposta:
        return False, "Campos vazios"
    
    # Verificar se n√£o √© s√≥ pontua√ß√£o
    if len(re.findall(r'\w+', resposta)) < 3:
        return False, "Resposta sem conte√∫do"
    
    # Verificar URLs suspeitas (poss√≠vel spam)
    if len(re.findall(r'http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+])+', resposta)) > 2:
        return False, "Muitas URLs"
    
    return True, "OK"

def limpar_dataset(dataset):
    """
    Limpa e valida dataset completo
    """
    dataset_limpo = []
    estatisticas = {'removidos': 0, 'mantidos': 0, 'motivos': {}}
    
    for exemplo in dataset:
        # Limpar textos
        if 'instrucao' in exemplo:
            exemplo['instrucao'] = limpar_texto(exemplo['instrucao'])
        if 'resposta' in exemplo:
            exemplo['resposta'] = limpar_texto(exemplo['resposta'])
        
        # Validar
        valido, motivo = validar_exemplo(exemplo)
        
        if valido:
            dataset_limpo.append(exemplo)
            estatisticas['mantidos'] += 1
        else:
            estatisticas['removidos'] += 1
            estatisticas['motivos'][motivo] = estatisticas['motivos'].get(motivo, 0) + 1
    
    # Relat√≥rio
    print("üßπ LIMPEZA DE DATASET")
    print("=" * 60)
    print(f"Total original: {len(dataset)}")
    print(f"Mantidos:       {estatisticas['mantidos']}")
    print(f"Removidos:      {estatisticas['removidos']}")
    
    if estatisticas['motivos']:
        print(f"\nMotivos de remo√ß√£o:")
        for motivo, count in estatisticas['motivos'].items():
            print(f"  {motivo}: {count}")
    
    return dataset_limpo

# Uso
dataset_limpo = limpar_dataset(dataset_original)
```

**3. Tokeniza√ß√£o e Comprimento**

```python
# analisar_tokens.py
from transformers import AutoTokenizer

def analisar_comprimento_tokens(dataset, model_name="meta-llama/Llama-2-7b-hf"):
    """
    Analisa comprimento em tokens (n√£o palavras!)
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    comprimentos = []
    muito_longos = 0
    max_length = 2048  # Limite t√≠pico
    
    for exemplo in dataset:
        texto_completo = formatar_prompt(
            exemplo['instrucao'],
            exemplo['resposta']
        )
        
        tokens = tokenizer.encode(texto_completo)
        comprimentos.append(len(tokens))
        
        if len(tokens) > max_length:
            muito_longos += 1
    
    import numpy as np
    
    print("üìè AN√ÅLISE DE COMPRIMENTO (TOKENS)")
    print("=" * 60)
    print(f"M√≠nimo:    {min(comprimentos)} tokens")
    print(f"M√°ximo:    {max(comprimentos)} tokens")
    print(f"M√©dia:     {np.mean(comprimentos):.0f} tokens")
    print(f"Mediana:   {np.median(comprimentos):.0f} tokens")
    print(f"P95:       {np.percentile(comprimentos, 95):.0f} tokens")
    
    if muito_longos > 0:
        pct = 100 * muito_longos / len(dataset)
        print(f"\n‚ö†Ô∏è  {muito_longos} exemplos ({pct:.1f}%) excedem {max_length} tokens")
        print(f"    Considera truncar ou remover estes exemplos")
    
    # Recomenda√ß√£o de max_length
    p95 = int(np.percentile(comprimentos, 95))
    max_recomendado = min(2048, ((p95 // 128) + 1) * 128)  # Arredondar a m√∫ltiplo de 128
    
    print(f"\n‚úÖ max_length recomendado: {max_recomendado}")
    
    return comprimentos, max_recomendado

# Uso
comprimentos, max_len = analisar_comprimento_tokens(dataset)
```

#### Template de Prepara√ß√£o Completa

```python
# preparar_dataset_completo.py
"""
Pipeline completo de prepara√ß√£o de dataset
"""

class PreparadorDataset:
    def __init__(self, formato='alpaca'):
        self.formato = formato
        self.estatisticas = {}
    
    def pipeline_completo(self, dados_brutos, output_file="dataset_preparado.jsonl"):
        """
        Pipeline completo: validar ‚Üí limpar ‚Üí balancear ‚Üí analisar ‚Üí guardar
        """
        print("üöÄ INICIANDO PREPARA√á√ÉO DE DATASET\n")
        
        # 1. Limpeza
        print("PASSO 1: Limpeza")
        dados_limpos = limpar_dataset(dados_brutos)
        print()
        
        # 2. Balanceamento
        print("PASSO 2: Balanceamento")
        dados_balanceados = balancear_categorias(dados_limpos)
        print()
        
        # 3. An√°lise de diversidade
        print("PASSO 3: An√°lise de Diversidade")
        metricas = analisar_diversidade(dados_balanceados)
        print()
        
        # 4. An√°lise de tokens
        print("PASSO 4: An√°lise de Tokens")
        comprimentos, max_len = analisar_comprimento_tokens(dados_balanceados)
        print()
        
        # 5. Split treino/valida√ß√£o
        print("PASSO 5: Split Treino/Valida√ß√£o")
        treino, validacao = split_dataset_correto(dados_balanceados)
        print()
        
        # 6. Guardar
        print("PASSO 6: Guardar Dataset")
        import json
        
        with open(f"train_{output_file}", 'w', encoding='utf-8') as f:
            for exemplo in treino:
                f.write(json.dumps(exemplo, ensure_ascii=False) + '\n')
        
        with open(f"val_{output_file}", 'w', encoding='utf-8') as f:
            for exemplo in validacao:
                f.write(json.dumps(exemplo, ensure_ascii=False) + '\n')
        
        print(f"‚úì Guardado em train_{output_file} e val_{output_file}")
        
        # Relat√≥rio final
        self.relatorio_final(treino, validacao, max_len)
        
        return treino, validacao
    
    def relatorio_final(self, treino, validacao, max_len):
        """
        Relat√≥rio final com recomenda√ß√µes
        """
        print("\n" + "=" * 60)
        print("üìã RELAT√ìRIO FINAL")
        print("=" * 60)
        print(f"Dataset de treino:   {len(treino)} exemplos")
        print(f"Dataset de valida√ß√£o: {len(validacao)} exemplos")
        print(f"max_length sugerido: {max_len}")
        
        # Estimativa de tempo
        print("\n‚è±Ô∏è  Estimativa de Treino (3 epochs, batch_size=4):")
        estimar_tempo_treino(len(treino), epochs=3, batch_size=4)
        
        print("\n‚úÖ Dataset pronto para treino!")
        print("   Pr√≥ximo passo: Fine-tuning com LoRA")

# Uso
preparador = PreparadorDataset(formato='alpaca')
train_data, val_data = preparador.pipeline_completo(dados_brutos)
```

#### Checklist de Qualidade do Dataset

```
‚úÖ QUALIDADE
   [ ] Respostas revistas manualmente (amostra de 10%)
   [ ] Sem erros ortogr√°ficos ou gramaticais
   [ ] Informa√ß√£o factualmente correta
   [ ] Tom e estilo consistentes

‚úÖ DIVERSIDADE
   [ ] T√≥picos variados
   [ ] Comprimentos diversos (curto, m√©dio, longo)
   [ ] Diferentes tipos de perguntas
   [ ] Vocabul√°rio rico

‚úÖ FORMATO
   [ ] Formato consistente (Alpaca/ChatML)
   [ ] Campos obrigat√≥rios preenchidos
   [ ] Encoding UTF-8
   [ ] Linha por exemplo (JSONL)

‚úÖ TAMANHO
   [ ] M√≠nimo de exemplos atingido (50-200 dependendo do caso)
   [ ] N√£o excessivamente grande (>10k pode ser contraproducente)
   [ ] Balanceado entre categorias

‚úÖ T√âCNICO
   [ ] Sem duplicados
   [ ] Split treino/valida√ß√£o correto (sem leakage)
   [ ] Comprimento de tokens analisado
   [ ] max_length definido apropriadamente

‚úÖ √âTICA
   [ ] Sem conte√∫do ofensivo ou discriminat√≥rio
   [ ] Sem informa√ß√£o pessoal identific√°vel
   [ ] Sem dados com copyright
   [ ] Consentimento para uso dos dados (se aplic√°vel)
```

---

#### Modelos at√© 7B par√¢metros

```bash
# Instalar depend√™ncias
pip install transformers==4.36.0
pip install peft==0.7.1              # Para LoRA
pip install bitsandbytes==0.41.3     # Para quantiza√ß√£o
pip install datasets
pip install accelerate
pip install trl                      # Para RLHF/SFT
```

#### Fine-tuning com LoRA - Exemplo Completo

**Dataset: Instru√ß√µes em Portugu√™s**

```python
# preparar_dataset_pt.py
from datasets import Dataset
import json

# Exemplo: Dataset de perguntas e respostas
dados_treino = [
    {
        "instrucao": "Explica o que √© aprendizagem autom√°tica",
        "resposta": "Aprendizagem autom√°tica √© um ramo da intelig√™ncia artificial que permite aos computadores aprender a partir de dados sem serem explicitamente programados..."
    },
    {
        "instrucao": "Como fazer um bolo de chocolate?",
        "resposta": "Para fazer um bolo de chocolate: 1. Pr√©-aquece o forno a 180¬∞C. 2. Mistura 200g de farinha..."
    },
    # ... mais exemplos
]

# Converter para formato de treino
def formatar_prompt(exemplo):
    """Formato Alpaca"""
    return f"### Instru√ß√£o:\n{exemplo['instrucao']}\n\n### Resposta:\n{exemplo['resposta']}"

# Criar dataset
dataset = Dataset.from_list(dados_treino)
dataset = dataset.map(lambda x: {"texto": formatar_prompt(x)})

print(f"‚úì Dataset criado com {len(dataset)} exemplos")
```

**Fine-tuning com LoRA (7B modelo):**

```python
# lora_finetuning.py
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset

class FineTunerLoRA:
    def __init__(self, modelo_base="meta-llama/Llama-2-7b-hf"):
        self.modelo_base = modelo_base
        self.device = "mps"  # Apple Silicon
        
    def carregar_modelo_4bit(self):
        """
        Carrega modelo em 4-bit para economizar mem√≥ria
        """
        print(f"üì• Carregando {self.modelo_base} em 4-bit...")
        
        # Configura√ß√£o de quantiza√ß√£o
        from transformers import BitsAndBytesConfig
        
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        
        # Carregar modelo
        model = AutoModelForCausalLM.from_pretrained(
            self.modelo_base,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True
        )
        
        # Preparar para treino
        model = prepare_model_for_kbit_training(model)
        
        # Tokenizer
        tokenizer = AutoTokenizer.from_pretrained(self.modelo_base)
        tokenizer.pad_token = tokenizer.eos_token
        
        print("‚úì Modelo carregado!")
        return model, tokenizer
    
    def configurar_lora(self, model):
        """
        Adiciona adaptadores LoRA ao modelo
        """
        print("üîß Configurando LoRA...")
        
        lora_config = LoraConfig(
            r=16,                    # Rank das matrizes LoRA (8-64)
            lora_alpha=32,           # Scaling factor
            target_modules=[
                "q_proj",
                "k_proj", 
                "v_proj",
                "o_proj",
                "gate_proj",
                "up_proj",
                "down_proj"
            ],
            lora_dropout=0.05,
            bias="none",
            task_type="CAUSAL_LM"
        )
        
        model = get_peft_model(model, lora_config)
        
        # Estat√≠sticas
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        all_params = sum(p.numel() for p in model.parameters())
        
        print("‚úì LoRA configurado!")
        print(f"  Par√¢metros trein√°veis: {trainable_params:,} ({100*trainable_params/all_params:.2f}%)")
        print(f"  Par√¢metros totais: {all_params:,}")
        
        return model
    
    def treinar(self, model, tokenizer, dataset, output_dir="./lora-model"):
        """
        Treina o modelo com LoRA
        """
        print("üöÄ Iniciando treino...")
        
        # Tokenizar dataset
        def tokenize(examples):
            return tokenizer(
                examples["texto"],
                truncation=True,
                max_length=512,
                padding="max_length"
            )
        
        tokenized_dataset = dataset.map(
            tokenize,
            batched=True,
            remove_columns=dataset.column_names
        )
        
        # Argumentos de treino otimizados para M1
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=4,      # Batch pequeno para M1
            gradient_accumulation_steps=4,       # Simula batch 16
            learning_rate=2e-4,
            fp16=True,                           # Mixed precision
            logging_steps=10,
            save_strategy="epoch",
            evaluation_strategy="epoch",
            warmup_steps=100,
            lr_scheduler_type="cosine",
            optim="adamw_torch",                 # Optimizador nativo PyTorch
        )
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False
        )
        
        # Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )
        
        # Treinar!
        trainer.train()
        
        # Guardar
        model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        print(f"‚úì Modelo guardado em {output_dir}")
        
        return model

# Uso completo
finetuner = FineTunerLoRA("meta-llama/Llama-2-7b-hf")
model, tokenizer = finetuner.carregar_modelo_4bit()
model = finetuner.configurar_lora(model)

# Carregar teu dataset
dataset = load_dataset("json", data_files="dados_treino.json")

# Treinar
model = finetuner.treinar(model, tokenizer, dataset["train"])
```

#### Testar Modelo Fine-tuned

```python
# testar_modelo.py
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

def testar_modelo_lora(base_model, lora_path, prompt):
    """
    Testa modelo fine-tuned com LoRA
    """
    # Carregar base
    print("üì• Carregando modelo...")
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        device_map="auto",
        torch_dtype=torch.float16
    )
    
    # Carregar adaptadores LoRA
    model = PeftModel.from_pretrained(model, lora_path)
    tokenizer = AutoTokenizer.from_pretrained(base_model)
    
    # Gerar resposta
    print(f"\nü§ñ Prompt: {prompt}\n")
    
    inputs = tokenizer(prompt, return_tensors="pt").to("mps")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.1
        )
    
    resposta = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"üìù Resposta:\n{resposta}")
    
    return resposta

# Teste
prompt = "### Instru√ß√£o:\nExplica o que √© o chip M1 da Apple\n\n### Resposta:"

resposta = testar_modelo_lora(
    base_model="meta-llama/Llama-2-7b-hf",
    lora_path="./lora-model",
    prompt=prompt
)
```

---

### 5.3 MLX Framework da Apple

#### O que √© MLX?

**MLX** = Machine Learning framework nativo da Apple para Apple Silicon
- Desenvolvido pela Apple Research
- Otimizado especificamente para M1/M2/M3
- Unified Memory aproveitada ao m√°ximo
- Sintaxe parecida com NumPy/PyTorch

**Vantagens no M1:**
- ‚úÖ Performance superior ao PyTorch MPS
- ‚úÖ Menos uso de mem√≥ria
- ‚úÖ Suporte nativo para quantiza√ß√£o
- ‚úÖ LLMs otimizados (llama, mistral, phi)

#### Instala√ß√£o e Setup

```bash
# Instalar MLX
pip install mlx
pip install mlx-lm  # Para modelos de linguagem

# Verificar instala√ß√£o
python -c "import mlx.core as mx; print(mx.__version__)"
```

#### Fine-tuning com MLX (Mais Eficiente!)

```python
# mlx_finetuning.py
"""
Fine-tuning com MLX - Mais r√°pido e eficiente que PyTorch no M1
"""

# Exemplo usando mlx-lm (wrapper de alto n√≠vel)
from mlx_lm import load, generate
from mlx_lm.tuner import train

# 1. Carregar modelo base
model, tokenizer = load("mlx-community/Mistral-7B-v0.1-4bit")

# 2. Preparar dados (formato JSONL)
"""
# dados.jsonl:
# {"text": "### Instru√ß√£o: ... ### Resposta: ..."}
# {"text": "### Instru√ß√£o: ... ### Resposta: ..."}
"""

# 3. Configura√ß√£o LoRA para MLX
config = {
    "lora_layers": 16,           # N√∫mero de camadas LoRA
    "lora_rank": 16,             # Rank
    "lora_alpha": 32,
    "batch_size": 4,
    "iters": 1000,               # Itera√ß√µes
    "learning_rate": 1e-5,
    "steps_per_eval": 100,
    "save_every": 100,
    "adapter_file": "adapters.npz"
}

# 4. Treinar
print("üöÄ Treinando com MLX...")
train(
    model="mlx-community/Mistral-7B-v0.1-4bit",
    data="dados.jsonl",
    valid_data="val.jsonl",
    **config
)

print("‚úì Treino completo! Adaptadores guardados em adapters.npz")

# 5. Testar
prompt = "### Instru√ß√£o: O que √© o MLX? ### Resposta:"
resposta = generate(
    model,
    tokenizer,
    prompt=prompt,
    max_tokens=200,
    temp=0.7
)
print(resposta)
```

#### Converter Modelos para MLX

```python
# converter_para_mlx.py
"""
Converter modelos HuggingFace para formato MLX
"""
from mlx_lm import convert

# Converter modelo
convert(
    hf_path="meta-llama/Llama-2-7b-hf",  # Modelo HuggingFace
    mlx_path="./llama-7b-mlx",           # Destino MLX
    quantize=True,                        # Quantizar para 4-bit
    q_group_size=64,                      # Tamanho do grupo
    q_bits=4                              # 4-bit quantization
)

print("‚úì Modelo convertido para MLX com quantiza√ß√£o 4-bit!")
```

#### MLX vs PyTorch no M1 - Compara√ß√£o

```python
# benchmark_mlx_vs_pytorch.py
import time
import mlx.core as mx
import torch

def benchmark_inferencia():
    """
    Compara velocidade MLX vs PyTorch
    """
    # MLX
    from mlx_lm import load, generate
    model_mlx, tokenizer_mlx = load("mlx-community/Mistral-7B-v0.1-4bit")
    
    prompt = "Explica o que √© intelig√™ncia artificial"
    
    # Teste MLX
    start = time.time()
    resposta_mlx = generate(model_mlx, tokenizer_mlx, prompt, max_tokens=100)
    tempo_mlx = time.time() - start
    
    # PyTorch MPS
    from transformers import AutoModelForCausalLM, AutoTokenizer
    model_pt = AutoModelForCausalLM.from_pretrained(
        "mistralai/Mistral-7B-v0.1",
        device_map="mps",
        torch_dtype=torch.float16
    )
    tokenizer_pt = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
    
    start = time.time()
    inputs = tokenizer_pt(prompt, return_tensors="pt").to("mps")
    outputs = model_pt.generate(**inputs, max_new_tokens=100)
    tempo_pt = time.time() - start
    
    print("=" * 60)
    print("BENCHMARK: MLX vs PyTorch MPS")
    print("=" * 60)
    print(f"MLX:        {tempo_mlx:.2f}s")
    print(f"PyTorch:    {tempo_pt:.2f}s")
    print(f"Speedup:    {tempo_pt/tempo_mlx:.2f}x")
    print("=" * 60)

# Executar
benchmark_inferencia()

# Resultado t√≠pico no M1 16GB:
# MLX:        3.2s
# PyTorch:    5.8s
# Speedup:    1.8x ‚úÖ
```