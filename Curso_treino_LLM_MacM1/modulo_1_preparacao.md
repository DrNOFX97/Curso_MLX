# Curso: Como Treinar Modelos de Aprendizagem AutomÃ¡tica (Machine Learning) no MacBook Pro M1 16GB

## MÃ³dulo 1: PreparaÃ§Ã£o do Ambiente

### 1.1 IntroduÃ§Ã£o ao chip Apple Silicon M1

#### Arquitetura ARM vs x86
O chip M1 da Apple representa uma mudanÃ§a fundamental na computaÃ§Ã£o pessoal:

**Arquitetura ARM:**
- O M1 usa arquitetura ARM64, nÃ£o x86 como os processadores Intel
- Mais eficiente energeticamente (RISC vs CISC)
- Alguns programas precisam de ser recompilados ou usar Rosetta 2 (camada de traduÃ§Ã£o)
- Para ML: bibliotecas nativas ARM tÃªm desempenho muito superior

**DiferenÃ§as prÃ¡ticas:**
- BinÃ¡rios x86 funcionam via Rosetta 2, mas sÃ£o mais lentos
- Python compilado para ARM Ã© atÃ© 2-3x mais rÃ¡pido
- Nunca uses Python do python.org (Ã© x86) - usa Miniforge!

#### GPU Integrada e Neural Engine

**GPU (7 ou 8 nÃºcleos no M1):**
- Totalmente integrada com a CPU (Unified Memory Architecture)
- Acesso direto Ã  RAM sem transferÃªncias CPUâ†”GPU
- Framework Metal para aceleraÃ§Ã£o GPU
- Ideal para treino de modelos pequenos/mÃ©dios

**Neural Engine (16 nÃºcleos):**
- Chip dedicado para operaÃ§Ãµes de ML
- 11 TFLOPS de desempenho
- Usado principalmente para inferÃªncia (Core ML)
- Menos flexÃ­vel que GPU, mas muito eficiente

#### Vantagens para Machine Learning

1. **Unified Memory Architecture (UMA):**
   - CPU e GPU partilham os mesmos 16GB de RAM
   - Sem overhead de transferÃªncias de dados
   - Batch sizes maiores que GPUs dedicadas com VRAM limitada

2. **EficiÃªncia EnergÃ©tica:**
   - Treino prolongado sem sobreaquecimento
   - Bateria dura horas mesmo em treino intensivo

3. **Desempenho Surpreendente:**
   - Para modelos atÃ© ~7B parÃ¢metros, competitivo com GPUs mid-range
   - Excelente para prototipagem e experimentaÃ§Ã£o

## Metal: A Base do Aceleramento em Chips Apple Silicon

### IntroduÃ§Ã£o ao Metal

O Metal Ã© a API grÃ¡fica e de computaÃ§Ã£o de baixo nÃ­vel da Apple, lanÃ§ada em 2014. Ao contrÃ¡rio de APIs de alto nÃ­vel, o Metal proporciona acesso quase direto Ã  GPU, minimizando overhead e maximizando o desempenho. Para treino de LLMs em Mac M1, o Metal Ã© **fundamental** porque:

- Ã‰ a Ãºnica forma de aceder Ã  GPU integrada nos chips Apple Silicon
- Permite aproveitar a arquitetura unificada de memÃ³ria (Unified Memory Architecture)
- Fornece aceleraÃ§Ã£o hardware para operaÃ§Ãµes de machine learning

## Metal vs. CUDA: DiferenÃ§as Chave

| Aspeto | Metal | CUDA |
|--------|-------|------|
| **Fabricante** | Apple | NVIDIA |
| **Plataformas** | macOS, iOS, iPadOS | Windows, Linux |
| **Linguagem Shaders** | Metal Shading Language (MSL) | CUDA C/C++ |
| **MemÃ³ria** | Unificada (CPU+GPU) | Separada (requer cÃ³pias) |

## Metal no Contexto do MLX

O MLX foi desenvolvido especificamente pela Apple para aproveitar o Metal:

```python
import mlx.core as mx

# O MLX usa Metal automaticamente
array = mx.array([1, 2, 3, 4])
# Esta operaÃ§Ã£o executa na GPU via Metal
resultado = array * 2
```

### Vantagens da Arquitetura Unificada

No M1, CPU e GPU partilham a mesma memÃ³ria RAM. Isto significa:

1. **Zero-copy operations**: NÃ£o hÃ¡ necessidade de transferir dados entre memÃ³rias
2. **Maior eficiÃªncia**: Menos latÃªncia e maior bandwidth
3. **Melhor uso de memÃ³ria**: Os 8-16GB sÃ£o partilhados inteligentemente

```python
# Com CUDA (NVIDIA):
# 1. Alocar memÃ³ria na GPU
# 2. Copiar dados CPU â†’ GPU
# 3. Processar
# 4. Copiar resultados GPU â†’ CPU

# Com Metal (M1):
# 1. Processar (dados jÃ¡ acessÃ­veis!)
```

## Metal Performance Shaders (MPS)

O PyTorch e outros frameworks usam MPS, uma camada sobre o Metal otimizada para ML:

```python
import torch

# Verifica se MPS estÃ¡ disponÃ­vel
if torch.backends.mps.is_available():
    device = torch.device("mps")
    tensor = torch.randn(1000, 1000).to(device)
    # OperaÃ§Ãµes aceleradas por Metal
    resultado = torch.matmul(tensor, tensor.T)
```

## LimitaÃ§Ãµes do Metal para ML

Apesar das vantagens, hÃ¡ limitaÃ§Ãµes a considerar:

1. **MemÃ³ria partilhada**: Os 8-16GB sÃ£o divididos entre CPU e GPU
2. **Menos maduro que CUDA**: Ecossistema menor, menos otimizaÃ§Ãµes
3. **Debugging**: Ferramentas menos desenvolvidas
4. **Compatibilidade**: Alguns modelos podem nÃ£o funcionar otimamente

**Ponto-chave**: O Metal nÃ£o Ã© apenas uma "alternativa ao CUDA" - Ã© uma arquitectura fundamentalmente diferente que aproveita as caracterÃ­sticas Ãºnicas dos chips Apple Silicon. Compreender esta diferenÃ§a Ã© essencial para otimizar o treino de LLMs no M1.

---

### 1.2 ConfiguraÃ§Ã£o Inicial

#### Passo 1: Instalar Homebrew

Homebrew Ã© o gestor de pacotes essencial para macOS. Abre o Terminal e executa:
```bash
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
```

ApÃ³s instalaÃ§Ã£o, adiciona ao PATH (o instalador mostrarÃ¡ os comandos exatos):
```bash
echo 'eval "$(/opt/homebrew/bin/brew shellenv)"' >> ~/.zshrc
eval "$(/opt/homebrew/bin/brew shellenv)"
```

Verifica a instalaÃ§Ã£o:
```bash
brew --version
```

#### Passo 2: Instalar Miniforge (Python para ARM)

**IMPORTANTE:** Nunca uses o Python do python.org no M1! Usa Miniforge (Conda otimizado para ARM).

```bash
# Download do Miniforge
brew install miniforge

# Inicializar conda
conda init zsh

# Reinicia o terminal ou executa:
source ~/.zshrc
```

Verifica que estÃ¡s a usar Python ARM:
```bash
python -c "import platform; print(platform.machine())"
# Deve retornar: arm64
```

#### Passo 3: Criar Ambiente Virtual Base

Cria um ambiente dedicado para AA:

```bash
# Criar ambiente com Python 3.11
conda create -n ml-m1 python=3.11 -y

# Ativar ambiente
conda activate ml-m1

# Instalar pacotes base
conda install -c conda-forge numpy pandas matplotlib scikit-learn jupyter -y
```

**Configurar ambiente como padrÃ£o:**
```bash
# Adiciona ao ~/.zshrc para ativar automaticamente
echo "conda activate ml-m1" >> ~/.zshrc
```

#### Passo 4: Ferramentas de Desenvolvimento

```bash
# Git (se ainda nÃ£o tiveres)
brew install git

# Editor de cÃ³digo (opcional)
brew install --cask visual-studio-code

# Ferramentas de monitorizaÃ§Ã£o
brew install htop
```


---

### 1.3 Frameworks Otimizados para M1

#### TensorFlow Metal

TensorFlow com aceleraÃ§Ã£o GPU via Metal:

```bash
conda activate ml-m1

# Instalar TensorFlow e plugin Metal
pip install tensorflow==2.16.1
pip install tensorflow-metal==1.1.0
```

**Testar aceleraÃ§Ã£o GPU:**

```python
import tensorflow as tf

# Verificar dispositivos disponÃ­veis
print("Dispositivos disponÃ­veis:")
print(tf.config.list_physical_devices())

# Deve mostrar GPU
print("\nGPU disponÃ­vel:", len(tf.config.list_physical_devices('GPU')) > 0)

# Teste de performance
import time
import numpy as np

# Criar dados aleatÃ³rios
x = tf.random.normal([10000, 10000])

# OperaÃ§Ã£o na GPU
start = time.time()
y = tf.matmul(x, x)
gpu_time = time.time() - start

print(f"\nTempo de multiplicaÃ§Ã£o de matrizes (10000x10000): {gpu_time:.4f}s")
print("âœ“ TensorFlow Metal estÃ¡ a funcionar!" if gpu_time < 1.0 else "âš  Performance abaixo do esperado")
```

#### PyTorch com MPS (Metal Performance Shaders)

PyTorch com suporte nativo para GPU do M1:

```bash
conda activate ml-m1

# Instalar PyTorch com suporte MPS
pip install torch torchvision torchaudio
```

**Testar aceleraÃ§Ã£o MPS:**

```python
import torch

# Verificar disponibilidade do MPS
print("MPS disponÃ­vel:", torch.backends.mps.is_available())
print("MPS construÃ­do:", torch.backends.mps.is_built())

# Definir dispositivo
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"Usando dispositivo: {device}")

# Teste de performance
import time

x = torch.randn(10000, 10000, device=device)

start = time.time()
y = torch.matmul(x, x)
torch.mps.synchronize()  # Esperar GPU terminar
mps_time = time.time() - start

print(f"\nTempo de multiplicaÃ§Ã£o de matrizes: {mps_time:.4f}s")
print("âœ“ PyTorch MPS estÃ¡ a funcionar!" if mps_time < 1.0 else "âš  Performance abaixo do esperado")
```

#### ComparaÃ§Ã£o: TensorFlow vs PyTorch no M1

| Aspecto | TensorFlow Metal | PyTorch MPS |
|---------|-----------------|-----------------|
| **Maturidade M1** | Muito estÃ¡vel | EstÃ¡vel (melhorou muito) |
| **Desempenho** | Excelente | Excelente |
| **Compatibilidade** | Alta | Algumas operaÃ§Ãµes nÃ£o suportadas |
| **Comunidade** | Maior | A crescer rapidamente |
| **RecomendaÃ§Ã£o** | Projetos de produÃ§Ã£o | Pesquisa e prototipagem |

#### JAX com Metal (Opcional - AvanÃ§ado)

A Apple lanÃ§ou suporte oficial para JAX com aceleraÃ§Ã£o Metal! Ideal para pesquisa e computaÃ§Ã£o numÃ©rica de alto desempenho.

**InstalaÃ§Ã£o:**

```bash
conda activate ml-m1

# Instalar JAX com plugin Metal da Apple
pip install jax-metal
```

Isto instala automaticamente o JAX e o plugin Metal otimizado.

**Testar aceleraÃ§Ã£o Metal:**

```python
import jax
import jax.numpy as jnp

# Verificar dispositivos
print("Dispositivos JAX:", jax.devices())
print("Dispositivo padrÃ£o:", jax.default_backend())

# Teste de performance
import time

x = jax.random.normal(jax.random.PRNGKey(0), (10000, 10000))

# JIT compile para otimizaÃ§Ã£o
@jax.jit
def matrix_multiply(a):
    return jnp.matmul(a, a)

# Warm-up (primeira execuÃ§Ã£o compila)
_ = matrix_multiply(x)

# Benchmark
start = time.time()
result = matrix_multiply(x)
result.block_until_ready()  # Esperar GPU terminar
jax_time = time.time() - start

print(f"\nTempo de multiplicaÃ§Ã£o de matrizes: {jax_time:.4f}s")
print("âœ“ JAX Metal estÃ¡ a funcionar!")
```

**Quando usar JAX:**
- Pesquisa que requer diferenciaÃ§Ã£o automÃ¡tica avanÃ§ada
- CÃ³digo cientÃ­fico que precisa de ser muito otimizado
- Quando queres controlo fino sobre transformaÃ§Ãµes (vmap, pmap, etc.)
- Treino com XLA (compilaÃ§Ã£o otimizada)

**DocumentaÃ§Ã£o oficial:** https://developer.apple.com/metal/jax/

#### VerificaÃ§Ã£o Final do Ambiente

Cria um script de teste completo:

```python
# teste_ambiente.py
import sys
print(f"Python: {sys.version}")
print(f"Arquitetura: {sys.platform}")

try:
    import tensorflow as tf
    print(f"âœ“ TensorFlow: {tf.__version__}")
    print(f"  GPU disponÃ­vel: {len(tf.config.list_physical_devices('GPU')) > 0}")
except ImportError:
    print("âœ— TensorFlow nÃ£o instalado")

try:
    import torch
    print(f"âœ“ PyTorch: {torch.__version__}")
    print(f"  MPS disponÃ­vel: {torch.backends.mps.is_available()}")
except ImportError:
    print("âœ— PyTorch nÃ£o instalado")

try:
    import numpy as np
    print(f"âœ“ NumPy: {np.__version__}")
except ImportError:
    print("âœ— NumPy nÃ£o instalado")

try:
    import pandas as pd
    print(f"âœ“ Pandas: {pd.__version__}")
except ImportError:
    print("âœ— Pandas nÃ£o instalado")

print("\nðŸŽ‰ Ambiente configurado com sucesso!")
```

Executa:
```bash
python teste_ambiente.py
```


---

### âœ… Checklist MÃ³dulo 1

- [ ] Homebrew instalado
- [ ] Miniforge instalado (Python ARM)
- [ ] Ambiente virtual `ml-m1` criado
- [ ] TensorFlow Metal instalado e testado
- [ ] PyTorch MPS instalado e testado
- [ ] Script de teste executado com sucesso
- [ ] GPU/MPS a funcionar corretamente


---

### ðŸŽ¯ PrÃ³ximos Passos

No **MÃ³dulo 2**, vamos aprender a gerir eficientemente os 16GB de RAM e monitorizar recursos durante o treino!

```
